---
title: "Regression Assumptions"
output:  html_notebook
---

# Overview

The OLS model encountered in the previous notebook is built on a series of assumptions that we will now examine.  We will also look at some of the tools at our disposal when one or more of the assumptions do not hold.  As usual, we start with loading in the data and libraries we will be using.  Some of these libraries are ones we are encountering for the first time; you may need to use the `install.packages()` function on them first.  Also, we won't be attaching the CPS1985 or wage1 datasets because many of the variable names in there clash with each other, which would make the analysis messy.

```{r, warning=FALSE, message = FALSE}
library(tidyverse)
library(stargazer)
library(sandwich)
library(lmtest)
library(wooldridge)
library(AER)
data(wage1)
data(CPS1985)
data(ceosal1)
data(nyse)
data(smoke)
data(vote1)
data(hprice3)
```

# The Key Assumptions

We will work through a series of assumptions upon which the OLS model is built, and what one might do if these assumptions do not hold. We will be encountering these assumptions throughout the rest of the semester!

## Assumption 1: The linear regression model is "linear in parameters"  

The basic model is:

\begin{equation}
Y_{i} = \alpha + \beta X_{i} + \epsilon_i
\end{equation}

This is the equation for a straight line.  But what if the data doesn't really look like a straight line.  Let's look at this data from the ceosal1 dataset in the wooldridge package.  Here, we graph CEO salary  on the Y axis and the company sales on the X axis.

```{r, message = FALSE}
ceosal1 %>% ggplot(aes(y = salary, x = sales)) + 
    geom_point() +
    theme_classic() +
    geom_smooth(method=lm)
```
So this doesn't say much, and I'd guess there is not much of a relationship when we estimate the regression.  But, part of what is going on might be due to the fact that both the CEO salary and sales data look skewed, so maybe we just don't have a linear relationship.  Let's see what the regression looks like:

```{r, warning = FALSE}
reg1a <- lm(salary ~ sales, data = ceosal1)
stargazer(reg1a, type = "text")


```
The result is significant at the 10%% level, $R^2 = .01$ is tiny, but we might be able to do better if we do something about the non-linearity. 

All OLS requires is that the model is linear in parameters.  We can do mathematical transformations of the data to create a linear function. Here, let's calculate the natural log of salary and sales plot that.

```{r}
tempdata <- ceosal1 %>% 
    mutate(lnsalary = log(salary)) %>% 
    mutate(lnsales = log(sales))
```

Now, let's take a look at the plot between lnsales and lnprice:

```{r, warning = FALSE}
tempdata %>% ggplot(aes(x = lnsales, y = lnsalary)) +
    geom_point() +
    theme_classic() +
  geom_smooth(method = lm)
    
```
Wow, quite a change.  This data looks like it might actually have a **linear** relationship now.

```{r, warning = FALSE}
reg1b <- lm(lnsalary ~ lnsales, data = tempdata)
stargazer(reg1b, type = "text")
```
The $R^2$ is considerably higher now, the $\beta$ is significant at the %1 level, and all in all this is a much more compelling model.  

The log transformation is probably the most common one we see in econometrics.  Not only because it is useful in making skewed data more amenable to linear approaches, but because there is a very useful interpretation of the results.  Let's look at the regressions again, side by side:

```{r, warning = FALSE}
stargazer(reg1a, reg1b, type = "text")
```
The regression on the left was the linear-linear model.  Interpreting this demands that we are aware of the units of measure in the data: sales are measured in millions of dollars, salary in thousands of dollars. So $\beta=0.015$ literally says that if sales goes up by 1, salary goes up by 0.015. But we interpret this as saying that, for every additional \$1,000,000 in sales, CEO salary is expected to go up by \$15.  This hardly seems plausible.  The model on the right is the log-log model, and says that a 1% increase in sales on average leads to CEO pay going up by 0.25%.  By log transforming a variable before estimating the regression, you change the interpretation from level increases into percentage increases.  

Let's look at the linear-log and log-linear models too.  

```{r, warning = FALSE}
reg1c <- lm(lnsalary ~ sales, data = tempdata)
reg1d <- lm(salary ~ lnsales, data = tempdata)
stargazer(reg1a, reg1d, reg1c, reg1b, type = "text")

```
Here all 4 specifications are side-by-side.  We've already interpreted columns 1 and 4.  How would we interpret columns 2 and 3?  

* Column 2 is the linear-log model.  A 1% increase in sales is associated with an increase in CEO salary of $262,901.

* Column 3 is the log-linear model.  A \$1,000,000 increase in sales is associated with a .001% higher salary.

The model in column 4 seems to be the best model of the bunch.  

Another common non-linear transformation is the quadratic transformation; this is particularly useful in cases where you think a relationship may be decreasing up to a point, and then start increasing after that point (or vice versa).  To see this in action, let's look at a graph that looks at the relationship between the age and selling prices of homes in the `hprice3` data from the `wooldridge` package.

```{r}
hprice3 %>% ggplot(aes(x = age, y = price)) +
    geom_point(color = "orange") +
    theme_classic() +
    labs(title = "House Value and Age") 
```
This relationship looks somewhat U-shaped; moving left to right, it seems that the value of houses falls as they get older, but at a certain point, the relationship reverses course and older becomes **more** valuable!

This is a great place to estimate a quadratic regression, which is just a fancy term for including both $age$ and $age^2$ in our regression.

\begin{equation}
Price_{i} = \alpha + \beta_1 age_{i} + \beta_2 age_i^2 + \epsilon_i
\end{equation}

We can either use the `I()` argument in our regression, or we can manually create a squared term and put it in our regression.  The hprice3 data already has a squared term in it called agesq, so let's verify that both methods get us to the same place:

```{r, warning = FALSE}
reg1e <- lm(price ~ age, data = hprice3)
reg1f <- lm(price ~ age + I(age^2), data = hprice3)
reg1g <- lm(price ~ age + agesq, data = hprice3)
stargazer(reg1e, reg1f, reg1g, type = "text")
```
Both columns 2 and 3 are the same, as expected.  Our regression model, then, looks like:

\begin{equation}
Price_{i} = \$113,762.10 - \$1691.90 age_{i} + \$9.26 age_i^2 + \epsilon_i
\end{equation}

We can look at these two models graphically as well: the orange line is the linear model (column 1 above), the purple line is the quadratic model (column 2/3 above).  The purple line is clearly a better fit.

```{r, message = FALSE}
hprice3 %>% ggplot(aes(x = age, y = price)) +
    geom_point(color = "darkorange3",
               alpha = .4) +
    theme_classic() +
    labs(title = "House Value and Age",
         y = "Price",
         x = "Age") +
    geom_smooth(method = lm, 
                color = "black",
                se = FALSE) +
    geom_smooth(method = lm, 
                formula = y ~ x + I(x^2),
                color = "brown",
                se = FALSE) +
    scale_y_continuous(labels=scales::dollar_format())
```
We can also, with a little bit of calculus, figure out the age at which the relationship stops decreasing and starts increasing.  You simply need to take the derivative of the regression equation with respect to age, set it equal to zero, and solve for age!

\begin{equation}
Price_{i} = \$113,762.10 - \$1691.90 age_{i} + \$9.26 age_i^2 + \epsilon_i
\end{equation}
\begin{equation}
\frac{\partial Price_i}{\partial age} = \$1691.90 + 2 \cdot \$9.26 age_i = 0 \: at \: age^\star
\end{equation}
\begin{equation}
\frac{\$1691.90}{\$18.52} = age^\star=91.4 
\end{equation}

As houses age, they lose value until they hit 91.4 years of age, at which point they start increasing in value! 

## Assumption 2: The average of the error term is 0

You may have wondered why we bother with having a constant term $\alpha$ in our regressions if nobody really cares about it.  It turns out that the constant term makes this assumption true.  For example. let's look back at our log-log model from the above:

```{r}
summary(reg1b)
```
The **Residuals** panel looks at the distribution of the error term.  Each residual from the regression is stored in the regression object; let's put them in our tempdata dataset and take a look at the first few rows.

```{r}
tempdata$resid <- reg1b$residuals
head(tempdata[c(1,3,13,14,15)],)
```
Is the mean of our residuals = 0?  

```{r}
mean(tempdata$resid)
```
That's about as close to zero as you can get.  

```{r}
format(mean(tempdata$resid), scientific = FALSE)
```
As long as you always have $\alpha$ in your regression, this assumption isn't something to worry about. There are only occasionally cases where you might want to run a regression without a constant, but they are rare.

## Assumption 3: The independent variable is uncorrelated with the error term

If the independent variable is correlated with the error term, it means the error isn't random.

Since we already put the residuals in the tempdata data frame, let's see if the residual is correlated with the independent variable lnsales.

```{r}
cor(tempdata$lnsales, tempdata$resid)
```
Pretty darn close to zero.  

Typically this error is violated when you have **confounding** variables or **omitted variables**, which basically means you need to add more independent variables to your model.  We already discussed this idea in the previous notebook.  

## Assumption 4: The error term is not serially correlated.

This is mostly a problem in time series regression (a regression where you use a continuous measure of time as an independent variable), and refers to a situation in which you can use error terms to predict each other.  Let's take a look at this data from the New York Stock Exchange from the wooldridge package--these data are about 12 years of Wednesday closing prices of the NYSE.  This data includes a pretty big market crash around observation 620 or so, which leads me to speculate that this data is from the 1980s and the huge drop is in October, 1987.  

```{r}
tempdata2 <- nyse 
tempdata2 %>% ggplot(aes(x = t, y = price)) +
    geom_point() +
    geom_line()
```
Next, let's estimate the regression and plot the residuals on the Y axis against time:
```{r}
reg4a <- lm(price ~ t, data = tempdata2)
tempdata2$resid <- reg4a$residuals
tempdata2 %>% ggplot(aes(x = t, y = resid)) +
    geom_point() +
    geom_line()
```
This graph exhibits what is called **serial correlation** or **autocorrelation** as the error terms are correlated with each other.  In other words, if we know the error term for week 1, we can use that to make a pretty good guess about the error term for week 2, and so forth. Again, the issues of serial correlation are mostly time series issues, so if we get to time series at the end of the semester, we will discuss these issues more then.

## Assumption 5: Homoskedasticity of the error term

We assume that the error term is **homoskedastic**, which means that the variance of the error term is not correlated with the dependent variable.  If the variance of the error term is correlated with the dependent variable, the data is said to be **heteroskedastic**.  We can look for heteroskedasticity by looking at a plot of residuals and fitted values.  

Let's take a look at a regression with homoskedasticity first.  We have looked before at the voting share data from vote1, here we estimate the regression and plot the fitted values on the X axis and the residuals on the Y axis.  For ease of reading, I am adding a horizontal line at 0:

```{r}
reg5a <- lm(voteA ~ shareA, data = vote1)
plot(reg5a$residuals ~ reg5a$fitted.values)
abline(a = 0, b = 0)

```

Note that the variation around the horizontal line is roughly the same for all of the possible fitted values.  Now, let's take a look at a regression using the smoke data in the wooldridge package.  We estimate the effect of income on the number of daily cigarettes smoked.  The estimated coefficients are not significant, but that's not important for what we are trying to show here.

```{r}
reg5b <- lm(cigs ~ income, data = smoke)
plot(reg5b$residuals ~ reg5b$fitted.values)
abline(a = 0, b = 0)

```

See how the shape of the residual plot looks a bit like a cone, with less spread on the left and a lot more spread on the right?  This is heteroskedasticity.  

For the most part, academic economists simply assume that heteroskedasticity is always a problem and as a matter of course just report **robust standard errors**.  In R, we can get this from the `sandwich` library.  If you haven't already, install and load the `sandwich` library and use `coeftest` function with the `vcovHC` option.

```{r, eval = FALSE}
install.packages("sandwich")
library(sandwich)
```
```{r}
coeftest(reg5b, vcovHC)
```
We can compare this result side-by-side with the original regression:

```{r, warning = FALSE}
stargazer(reg5b, coeftest(reg5b, vcovHC), type = "text")

```
It's a bit tough to see here, but the coefficients didn't change at all, only the standard errors (the numbers in the parentheses) changed.  

If a model is not **heteroskedastic** and doesn't have **autocorrelation**, it is said to have **spherical errors** and the error terms are **IID** (Independent and Identically Distributed).

## Assumption 6: No independent variable is a perfect linear function of other explanatory variables.

This one is important and will probably create quite a few headaches for you when we get to regression with categorical independent variables.  Let's introduce the concept quickly here though.  

As discussed in a previous notebook, ordinary least squares works by attributing the variation in the dependent variable Y to the variation in your independent variables.  If you have more than one independent variable, OLS needs to figure out which independent variable to attribute the variation to.  If you have two identical independent variables, R cannot distinguish one variable from the other when trying to apportion variation.  If you attempt to estimate a model that contains independent variables that are perfectly correlated, R will attempt to thwart you.  Typically, the way to proceed is to simply remove one of the offending variables.  

For example:

```{r}
reg6a <- lm(voteA ~ shareA + shareA, data = vote1)
stargazer(reg6a, type = "text")
```
R doesn't even let me run this--note that shareA is only included in the table once.  So let's trick it into running a regression with two identical variables:

```{r, error = TRUE}
tempdata3 <- vote1
tempdata3$shareAclone <- tempdata3$shareA
reg6b <- lm(voteA ~ shareA + shareAclone, data = tempdata3)
stargazer(reg6b, type = "text")
summary(reg6b)
```
Now R is quite displeased with us.  R simply dropped the shareAclone variable because it is impossible to run a regression with both shareA and shareAclone.

Let's dig a little deeper into the **linear function** idea.  Let's say you are running a regression with 3 independent variables, $X_1$, $X_2$, and $X_3$.

\begin{equation}
Y = \alpha + \beta_1 X_{1} + \beta_2 X_{2} +\beta_3 X_{3} +\epsilon_i
\end{equation}

This assumption basically states that:

* $X_1$, $X_2$, and $X_3$ are all different variables.
* $X_1$ is not simply a rescaled version of $X_2$ or $X_3$.  For example, If $X_1$ is height in inches, $X_2$ can't be height in centimeters because then $X_2 = 2.5X_1$
* $X_1$ cannot be reached with a linear combination of  $X_2$ and $X_3$. So, if $X_1$ is income, $X_2$ is consumption, and $X_3$ is savings, and thus $X_1 = X_2 + X_3$, you can't include all 3 variables in your equation. This is true of more compicated linear combinations as well; if $X_1 = 23.1 + .2X_2 - 12.4X_3$, you couldn't run that either.  

Why this typically causes trouble for people new to regression, however, is that they are not usually aware that there is another variable hidden in the regression, $X_0$, which carries a value of 1 for every observation.  This is technically what the $\alpha$ is multiplied by.  This means that $X_1$, $X_2$, and $X_3$ **cannot** be constants, because otherwise you will violate this assumption. 

Let's see what happens when we include another constant in the voting model:

```{r}
tempdata3$six <- 6
reg6c <- lm(voteA ~ shareA + six, data = tempdata3)
summary(reg6c)
```
R didn't like the constant in the regression and just chucked it out. 

Remember this lesson for when we start talking about dummy variable regressions!

A related issue you might run into is **multicollinearity**, which is where you don't have perfectly correlated independent variables but they are very, very close.  If these correlations are high enough, they generally cause problems. Let's see what happens.  Here, I will use the voting data and create a new variable called shareArand which is the value of shareA plus a random number between -1 and 1. 

```{r}
set.seed(8675309)
tempdata4 <- tempdata3 %>% 
    mutate(shareArand = shareA + runif(173, min = -1, max = 1))
cor(tempdata4$shareA, tempdata4$shareArand)
```
You can see that shareA and shareArand are very highly correlated.  What happens when I run this regression? Weird stuff:

```{r, warning = FALSE}
reg6d <- lm(voteA ~ shareA + shareArand, data = tempdata4)
reg6e <- lm(voteA ~ shareArand, data = tempdata4)
stargazer(reg6a, reg6d, reg6e, type = "text")
```
Compare the model with the multicollinearity on the right with the original model on the left. The coefficients are huge in absolute value compared to column 1.  One of the coefficients has the wrong sign.  And if you add the two $\beta$s in column 2 together, you get a number very close to the coefficient on shareA in column 1.  This is typical of models with multicollinearity.

Solving multicollinearity is pretty easy.  Just drop one of the collinear variables and the problem is solved.

## Assumption 7: Normality of Error Terms

The last assumption of the regression model is that your error terms are normally distributed.  Violating this assumption is not terrible, but if this assumption is violated it is often a sign that your models might be heavily influenced by outliers.  An easy way to look for this is the Q-Q plot. Let's look at a Q-Q plot of the voting regression:

```{r}
reg7a <- lm(voteA ~ shareA, data = vote1)
qqnorm(reg7a$residuals)
qqline(reg7a$residuals)
```

This Q-Q plot is pretty close to the line, indicating the residuals have pretty close to a normal distribution.  

Let's look at the Q-Q plot from the CEO salary regressions from up above:

```{r}
qqnorm(reg1b$residuals)
qqline(reg1b$residuals)
```

Not quite as close to the line, but still not bad.  

The Q-Q plot (and the residual plot from assumption 5) can be obtained another way; if you plot a regression object, you get 4 diagnostic plots, two of which are the ones we've looked at.

```{r}
plot(reg7a)
```

# Wrapping Up

We have examined the major assumptions that underlie the OLS regression model; keep these in the back of your mind as we progress through this semester because many of the advanced techniques we use are a result of one or more of these assumptions being violated.

We will next turn to expanding the power of multiple regression modeling to include categorical independent variables and interaction effects.