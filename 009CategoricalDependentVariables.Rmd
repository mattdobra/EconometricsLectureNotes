---
title: "Categorical Dependent Variables"
output: html_notebook
---

# Overview

Until now, we have been limited to using numeric variables as our dependent variables.  In this notebook we look at how we can use OLS modeling for categorical dependent variables with the **linear probability model**, as well as the related **logit** and **probit** models.  This set of models is designed for looking at **dichotomous** dependent variables, variables with only 2 possible outcomes.  These types of outcomes are of the sort that can be answered as a yes or no type question, for example:

* Was a job candidate hired?
* Did a customer make a purchase?
* Was a patient cured?
* Did a product break?
* Did a politician vote for a specific bill?

As you might be starting to suspect, there are tons of interesting questions that can be answered with these types of models! 

While not often utilized in economics, there are versions of probit and logit that are designed for **multinomial** (categorical variables with more than 2 outcomes) and **ordered** (multinomial variables whose levels can be ordered from worst to best, lowest to highest, etc.) dependent variables.  

Let's begin by loading in some essential packages.  We have not yet used `margins` or `jtools` so you may need to install those now.  Additionally, `jtools` requires a couple more packages to be installed to do what we will be asking it to do, `broom` and `huxtable`:


```{r, eval = FALSE}
install.packages("margins")
install.packages("jtools")
install.packages("broom")
install.packages("huxtable")
install.packages("mlogit")
```

```{r, warning=FALSE, message = FALSE}
library(tidyverse)
library(stargazer)
library(margins)
library(mlogit)
library(jtools)
library(wooldridge)
library(AER)
data(k401ksubs)
data(SmokeBan)
data(affairs)

data(CPS1985)
data(HousePrices)
data(diamonds)
data(gpa2)
data(TeachingRatings)
data(CPS1988)


```


# Linear Probability Model

The linear probability model (LPM) is the easiest place to begin -- while the model is generally thought to be inferior to probit or logit, estimating and interpreting the model is far more straightforward because it essentially just an OLS regression with a dummy variable as our dependent variable.  

Let's start with a simple bivariate model; it should help develop the intuition of predicting dichotomous outcomes without getting too complicated too quickly.  We will use the `k401ksubs` dataset in the `wooldridge` package, which contains 9275 individual level observations.  We will start by seeing whether or not income level, `inc` is predictive of whether or not an individual participates in a 401k account, captured in the `p401k` variable.

Let's start by just putting the two variables we are going to analyze into a smaller dataframe called `retdata` and look at some summary statistics.

```{r}
retdata <- k401ksubs %>% select(inc, p401k)
summary(retdata)
```
How do we interpret the results of the `p401k` variable? Because it is coded as a dummy variable, `p401k=1` tells us that an individual has a 401k account, and `p401k=0` tells us they do not. The mean of `r mean(retdata$p401k)` tells us that 27.6% of the individuals in the data set have a 401k account.  We can look to see if there is a different mean income of the two groups:

```{r, message = FALSE}
retdata %>% group_by(p401k) %>% 
    summarize(mean = mean(inc))
```

As a reminder, we could have gotten the same summary statistics using `mean(retdata$inc[retdata$p401k == 0])` and `mean(retdata$inc[retdata$p401k == 1])`.  These results do suggest that individuals who participate in their company's 401k program tend to have higher incomes on average.  

The **linear probability model** is simple to estimate; we run a regression using `p401k` as our dependent variable with `inc` as the sole independent variable.  

Luckily, the `k401ksubs` data has coded the `p401k` variable as a dummy for us already; if it were not and instead was coded as a factor, we would need an extra step (e.g. we could create a dummy variable using the `ifelse()` command, we could use the `as.numeric()` command, etc.). We will see examples of this sort later!  

```{r, warning = FALSE}
reg1a <- lm(p401k ~ inc, data = k401ksubs)
stargazer(reg1a, type = "text")
```

What does this result tell us?  The $\hat{\beta} = .005$ tells us that, for every \$1,000 increase in income, the expected probability of an individual participating in the 401k program increases by half a percentage point.  

\begin{equation}
Probability\:of\:401k\:Participation_{i} = .079 + .005 \cdot income_i 
\end{equation}

Just as with a regression, we can calculate a predicted probability by plugging in values for $income_i$; for example, we would estimate that somebody with \$50,000 income ($inc_i=50$), as:

\begin{equation}
Probability\:of\:401k\:Participation_{i} = .079 + .005 \cdot 50 = 32.9\% 
\end{equation}

What if we wanted to predict the probability of 401k participation for somebody with an income of \$190,000 ($inc_i=190$)

\begin{equation}
Probability\:of\:401k\:Participation_{i} = .079 + .005 \cdot 190 = 102.9\% 
\end{equation}

Somehow the model gives this individual a greater than 100% chance of participating in their company's 401k program, which is a problem, especially considering that $inc_i=190$ isn't extrapolation.

```{r, message = FALSE}
k401ksubs %>% ggplot(aes(x = inc, y = p401k)) +
    geom_point() +
    geom_smooth(method = lm)
```

A second problem with the LPM becomes apparent when examining the Q-Q plot.  Recall, the Q-Q plot examines the residuals of the model, and the Q-Q plot is expected to be a relatively straight line.  For a the linear probability model, it will almost certainly never be so:


```{r}
qqnorm(reg1a$residuals)
qqline(reg1a$residuals)
```
This is a funky looking graph, but this level of heteroskedasticity is completely expected for a linear probability model; you are predicting a probability, so most of your predictions are between 0 and 1.  However, the outcome variable `p401k` can ONLY be 0 or 1. This pretty much guarantees that you will have too many huge outliers.

Estimating a **logit** or **probit** model mitigates both of these problems.  Estimating these models are fairly straightforward and require the `glm()` command; GLM stands for **Generalized Linear Model**.  Let's estimate both the logit and probit models:

```{r}
reg1b <- glm(p401k ~ inc, family = binomial(link = "logit"), data = k401ksubs)
reg1c <- glm(p401k ~ inc, family = binomial(link = "probit"), data = k401ksubs)
```
The syntax for these are very similar to the syntax for `lm`; the function is specified in the same way (`p401k ~ inc`), the data is specified in the same way (`data = k401ksubs`), the only difference is that you need to specify the `family` option.  

While estimating the model is fairly straightforward, interpreting the results is not.  Let's look at the 3 results, side-by-side:

```{r, warning = FALSE}
stargazer(reg1a, reg1b, reg1c, type = "text")
```
Some things stand out.  The $R^2$ in this model has been replaced by the log likelihood and the AIC, and the estimated $\hat{\alpha}$ and $\hat{beta}$ look very different from those of the linear model.  Interpreting $\hat{\alpha}$ and $\hat{beta}$ in this form is quite difficult; it requires wrapping ones mind around the concept of log-likelihood and odds ratios and other such things.  If you are just interested in predictive modeling, these results are sufficient. But for the sake of learning how to interpret the model, we will learn how to convert these coefficients to a format that is interpreted like OLS coefficients shortly.

First though, let's look at a graph of what we just estimated.  

```{r, message = FALSE}
k401ksubs %>% ggplot(aes(x = inc, y = p401k)) +
    geom_point() +
    geom_smooth(method = lm, color = "blue") +
    stat_smooth(method = "glm", 
                method.args = list(family = binomial(link = "probit")),
                color = "red") +
    stat_smooth(method = "glm", 
                method.args = list(family = binomial(link = "logit")),
                color = "darkgreen") 

```
The blue line is the linear model that we plotted earlier, the red and green lines are the probit and logit models respectively.  Some points of note:

* The probit and logit models are very similar to each other.  Logit is often preferred in disciplines like biostats, because the logit coefficients have a useful interpretation in terms of odds ratios.  Generally speaking, there may be a slight preference for probit in economics, but for the most part, it doesn't much matter which one you use.
* The probit and logit models are clearly non-linear, and they will asymptotically approach 0 and 1 but will never actually hit 0 or 1. 

The fact that the model is non-linear provides a useful place to think about the concept of the **margin**.  In economics, the idea of "marginal" is very similar to the idea of "additional"; for example, the marginal cost of production is the additional cost incurred from producing one more item, marginal utility is the additional utility received from consuming a tiny bit more, and so on.  With that in mind, the estimated values of $\hat{\beta}$ from an OLS regression have a very similar interpretation -- a small, one unit change in your $X$ variable leads to a $\hat{\beta}$ sized increase in your $Y$ variable.  In the linear model, the value of $\hat{\beta}$ is constant; that is, regardless of the value of $X$, the marginal effect of $X$ on $Y$ is the same: $\hat{\beta}$. In a non-linear model like probit or logit, the marginal effect of $X$ on $Y$ varies by the value of $X$; where the curve is relatively flat, the marginal effect of $X$ on $Y$ is relatively small, whereas when the curve is relatively steep, the marginal effect of $X$ on $Y$ is relatively large. 

The upshot of all of this is that it takes some work to be able to interpret our probit and logit estimates in the same way we interpret our OLS estimates. To do so, we need to obtain **marginal effects** from our probit and logit model. Luckily, there are multiple packages we can install that will do all the work for us.  We will use the `margins` package.

There are two primary methods of calculating marginal effects:

* **Marginal Effect at the Means (MEM)** -- find the mean of all of your independent variables, and calculate the marginal effect of each variable at that point.
* **Average Marginal Effect (AME)** -- find the marginal effect for every observation in the data, and calculate the mean of those marginal effects.

MEM is easier to calculate, though in my opinion AME is the more generally appropriate method.  By default, `margins` uses the AME option.  The syntax for the `margins()` function is quite simple -- simply feed it a logit or probit object.

Unfortunately, `stargazer` does not work well with the output of the margins command, so we will switch to use `export_summs()` from the `jtools` package.  

```{r}
reg1bmargins <- margins(reg1b)
reg1cmargins <- margins(reg1c)
export_summs(reg1a, reg1bmargins, reg1cmargins)
```

One drawback of `export_summs()` is that the statistical significance stars don't match those of `stargazer` (and therefore are not those standard in economics), though this can be manually changed by adding a `stars` option. Another issue is that it does not identify the number of observations in the model correctly.  Finally, I will also add a couple options to aid with readability: naming the models, and changing the number format to add a couple of decimal places so we aren't looking at a table full of zeroes.  With probit and logit it is very useful to know how to use the `number_format` option, since the phenomenon we are measuring is a probability.  

```{r, warning = FALSE}
export_summs(reg1a, reg1bmargins, reg1cmargins, 
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1),
             model.names = c("OLS", "Logit", "Probit"))
```
Here we see that the estimated marginal effects of the logit and probit model are very similar to each other.   It is also of note that there is no intercept term reported for the logit or probit; this is expected because it is impossible to have a marginal change in the constant!  The estimated marginal effects are also quite a bit smaller than the OLS estimates; are they more accurate?  One way to compare probits/logits to OLS is by using the **Akiake Information Criterion (AIC)** with the `AIC()` command--lower scores are better.  A good rule of thumb for information criteria is that a difference of 10 is a big difference.  


```{r}
AIC(reg1a)
AIC(reg1b)
AIC(reg1c)

```
The probit and logit have very similar values for AIC, and they are significantly lower than that of the regression model, indicating that either model is significantly better than the OLS estimates.

# More examples using probit and logit

As the probit and logit models are a) generally a better fit than the linear probability model, and b) mostly a matter of preference, we will continue the rest of this notebook with probit models.  

As with OLS regressions, we can include multiple independent variables (though it's usually not a good idea to include interaction effects), non-linear terms, dummy variables, and so forth.  Let's estimate a probit with a quadratic term for income and `pira`, a dummy variable that is equal to 1 if the individual sa an IRA:

\begin{equation}
Probability\:of\:401k\:Participation_{i} = \alpha + \beta_1 \cdot income_i + \beta_2 \cdot income_i^2 + \beta_3 \cdot pira_i
\end{equation}

The following code will estimate this regression and display it with the probit from up above: 

```{r, message = FALSE, warning = FALSE}
reg1d <- glm(p401k ~ inc + I(inc^2) + pira, data = k401ksubs, family = binomial(link = "probit"))
reg1dmargins <- margins(reg1d)
export_summs(reg1cmargins, reg1dmargins,  
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1))

```
The estimated $\hat{\beta}=0.0515$ on the $pira$ dummy variable is relatively straightforward to interpret; all else equal, the probability than an individual with a value of $pira=1$ has a 401k is 5.15 percentage points higher than an individaual with a value of $pira=0$. 

The presentation of the income variable may be a bit of a surprise, however; wasn't there a squared term in the original model?  If we look at the original regression results (i.e. before we transformed them into marginal effects), we can see that the model was indeed estimated with a squared term!

```{r, warning = FALSE}
export_summs(reg1d, 
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1))
```

Not only was it there, but it was significantly less than zero, suggesting diminishing returns to income!  So where did the squared term go in the report from the `margins()` command?  Because we specified the model using the `I()` function, R knew that the $inc$ and $inc^2$ terms in the model are the same variable, so the `margins()` command knows to combine the two when calculating the marginal effect.  

Let's turn to a different data set, `SmokeBan` in the `AER` package.  We will use a probit model to predict whether or not is a smoker based on their age, gender, race, education, and whether or not there is a workplace smoking ban.  

As before, the scripting workflow here is to:

* store the results of a `glm()` in an object;
* run that regression object through the `margins()` function, and save that as an object;
* display the object created by `margins()` with `export_summs()`.

```{r, message = FALSE, warning = FALSE}
reg2a <- glm(smoker ~ ban + age + education + afam + hispanic + gender, data = SmokeBan, family = binomial(link = "probit"))
reg2amargins <- margins(reg2a)
export_summs(reg2amargins,  
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1))
```

This is another model in which there are a considerable number of dummy explanatory variables; as with OLS, we interpret them with respect to the omitted group.  For example, $\hat{\beta}=-0.0329$ for the female dummy indicates that females are 3.3 percentage points less likely to smoke than males.  It seems that the strongest predictor of smoking is education; we can see this nicely in graphic form with a stacked bar chart:

```{r}
SmokeBan %>% ggplot(aes(fill = smoker, x = education)) +
    geom_bar(position = "fill") +
    theme_classic()
```
Do smoking bans reduce smoking?  The estimated coefficient on smoking bans $\hat{\beta}=-0.0455$ suggests that perhaps they do, hough there may be questions of reverse causation here; perhaps non-smokers are more willing to work at non-smoking workplaces than are smokers.  

Let's take another look at the affairs dataset.  We predict whether or not an individual had an estramarital affair with gender, whether or not they have kids, education level, length of marriage, and measures of religiosity and their rating of their marital happiness. 

```{r}
reg3a <- glm(affair ~ male + yrsmarr + kids + relig + educ + ratemarr, data = affairs, family = binomial(link = "probit"))
reg3amargins <- margins(reg3a)
export_summs(reg3amargins, 
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1))
```
As one might expect, happily married and highly religious individuals are less likely to have an affair than otherwise.  If we look more deeply at the data descriptions with `data(affairs)`, we see that the religiosity and marital happiness variables are coded as such:

* relig: 5 = very relig., 4 = somewhat, 3 = slightly, 2 = not at all, 1 = anti
* ratemarr: 5 = vry hap marr, 4 = hap than avg, 3 = avg, 2 = smewht unhap, 1 = vry unhap

As these are coded as numbers, running the probit model with the variables as-is implies that the variables are somehow **measures**; our closer look, however, suggests that this data is actually **ordinal**, and therefore are **factors**, not numeric.  As such, it may make sense to ask R to treat these variables like factor variables instead of numbers, which we will do with the `as.numeric()` function.


```{r, warning = FALSE}
reg3b <- glm(affair ~ male + yrsmarr + kids + as.factor(relig) + educ + as.factor(ratemarr), data = affairs, family = binomial(link = "probit"))
reg3bmargins <- margins(reg3b)
export_summs(reg3amargins, reg3bmargins, 
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1),
             model.names = c("Measured", "Factors"))
```

Above, the two models are displayed side-by-side.  If we inspect the $\hat{\beta}$ coefficients on both the religiosity and marriage happiness variables, we see that there may be a little more nuance revealed when treating them with the `as.factor()` command than simply treating them like measured variables.  For example, there does not seem to be a difference between $relig = 4$ and $relig = 5$, nor does there appear to be a difference between $ratemarr = 1$ and $ratemarr = 2$, and $relig = 3$ seems to be worse for affairs than $relig = 2$. So which model is better?  Sorting this question out is a good place to use the information criteria.  The two models are very similar in terms of AIC

They have similar measures of AIC, but the **Bayesian Information Criterion (BIC)** is actually better for the model on the left by a pretty big margin: `r BIC(reg3a)` - `r BIC(reg3b)` = `r BIC(reg3a) - BIC(reg3b)` is considerably more than a difference of 10.  The AIC and BIC work a lot like Adjusted $R^2$ in regression in that they "penalize" you for having more independent variables than you need to model the data.  In this context, it means that even though the model on the right explains *more* of the variation in the dependent variable, it does so using a lot more information, and the information isn't adding enough to the predictive power of the model to justify its inclusion.

Let's work through one more probit example to see how we can estimate marginal effects for specific values of our independent variables.  This is especially important for calculating the marginal effects of interaction effects.  

Let's use the `recid` data in the wooldridge package.  This data looks at prison recidivism, and the variable `cens` indicates whether or not an individual released from prison wound up back in prison ($cens = 0$) or did not ($cens = 1$) at some point in the next 6-7 years.  Let's suppose we believe that  recidivism is influenced by whether or not an individual was in the North Carolina prison work program, and that we suspect the effectiveness of the prison work program is somehow affected by years of education. We will model $cens$ as a function the interaction between the prisoner's years of education and whether or not the participated in the North Carolina prison work program and other control variables.  

```{r}
reg4a <- glm(cens ~ educ*workprg + age +I(age^2) + drugs + alcohol + black + tserved,
             data = recid,
             family = binomial(link = "probit"))
export_summs(reg4a)
```
We see that all of our control variables are significant, but none of our variables of interest are.  Next, let's run the regression object through the `margins()` function to see some easier-to-interpet results:

```{r, warning = FALSE}
reg4amargins <- margins(reg4a)
export_summs(reg4amargins,
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1))
```
Of note is that both the quadratic term on $age$ and the interaction effect are not presented here; again, this is to be expected because of the way that marginal effects are calculated.  

Finally, to test our hypothesis that there is an interaction between the work program and education, we will use the `at` option in `margins()`.  Let's look at the marginal effects for $educ = 8$, $educ = 12$, and $educ = 16$:

```{r, warning = FALSE}

reg4amargins0 <- margins(reg4a, at = list(educ = 8))
reg4amargins1 <- margins(reg4a, at = list(educ = 12))
reg4amargins2 <- margins(reg4a, at = list(educ = 16))
# reg4amargins2 <- margins(reg4a, at = list(educ = 16))
export_summs(reg4amargins, reg4amargins0, reg4amargins1, reg4amargins2, 
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1),
             model.names = c("AME", "Education = 8", "Education = 12", "Education = 16"))

```
As we can see, estimating the marginal effect at differing levels of education reveals that the North Carolina prison work program seems to have a negative and significant effect on individuals with low levels of education.  It might also be instructive to estimate the marginal effects for $workprg = 0$ and $workprg = 1$: 

```{r, warning = FALSE}

reg4amarginsnowork <- margins(reg4a, at = list(workprg = 0))
reg4amarginswork <- margins(reg4a, at = list(workprg = 1))
export_summs(reg4amargins, reg4amarginsnowork, reg4amarginswork, 
             number_format = "%.4f",
             stars = c(`***` = 0.01, `**` = 0.05, `*` = 0.1),
             model.names = c("AME", "No Work Program", "Work Program"))
```
Looked at this way, we can see that education is not a good predictor of recidivism for individuals not in the work program, but it is for individuals in the work program.  

### Data for further exploration

* `AER:CreditCard` - model $card$, a binary variable denoting whether or not a credit card application was accepted.
* `AER:HMDA` - model $deny$, looking in particular for discrimination against black borrowers (the $afam$ dummy). Does discrimination go away when controlling for all relevant factors?
* `AER:PSID1976` - model $participation$ as a function of education, age, number of children, family income, etc.
* `AER:ResumeNames` - model $call$, looking in particular for discrimination against individuals with "black names." This type of analysis is called an "audit study"
* `wooldridge:card` - model $enroll$, a binary measuring whether or not a student enrolled at a college.

# Multinomial Logit

Using the `AER:TravelMode` data, we can estimate a multinomial logit using the `mlogit` package.