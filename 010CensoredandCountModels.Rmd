---
title: "Methods for Censored and Count Data"
output: html_notebook
---

# Overview

We now turn to methods that are appropriate for dependent variables that are either **censored** or **count** data.  This will include the **Tobit**, **Heckman**, **Poisson**, and **Negative Binomial** models.

As usual, we begin with loading in some essential packages.  We have not yet used `censReg`, `MASS`, or `sampleSelection`, so they may need to be installed prior to their use.  


```{r, eval = FALSE}
install.packages("censReg")
install.packages("sampleSelection")
```

```{r, warning=FALSE, message = FALSE}
library(tidyverse)
library(stargazer)
library(MASS)
library(censReg)
library(sampleSelection)
library(MASS)
library(wooldridge)
library(AER)

data(charity)
data(fringe)
data(mroz)
data(affairs)
data(Medicaid1986)
data(DoctorVisits)
data(EquationCitations)
data(NMES1988)
data(wagepan)
data(murder)
data(NaturalGas)
data(HealthInsurance)
```

# Censored Models

We will look at two types of models to fit censored data: the **Tobit** model and the **Heckman** model.  Unlike Probit and Logit, these re not interchangeable, and which one you use depends on the type of censoring involved.  A censored variable is a variable that, for some reason, cannot take on a value below (left-censored) or above (right-censored) some value, or is missing for some observations. This might be due to "corner solutions" or top/bottom coding. For example:

* If you are modeling demand for a sporting event, there is a maximum limit for how many tickets can be sold--the corner solution is the stadium capacity.
* Corner solutions are often common when the dependent variable is a percentage, so obversations get bunched at 0% and/or 100%.
* If you are trying to predict SAT scores, they are top-coded at 800 and bottom-coded at 200.  
* Data may not exist for some observations in your data set, and you think that the reason the data does not exist can be modeled.  

The first three such cases would call for a **Tobit** model, the last one would be a situation for a **Heckman** model.

## The Tobit model

Let's illustrate the Tobit model using the `fringe` dataset in the `wooldridge` package.  The variable $annbens$ contains the dollar value of annual benefits for the 616 individuals in the data set.

```{r}
summary(fringe$annbens)
```

Here, we can see that the minimum is \$0 and the maximum is \$5129.1.  Let's think about this a little further: 

* Does the maximum *have* to be \$5129.1?  Could it be higher? 
* Does the minimum *have* to be \$0?  Could it be lower?  

While it seems possible that somebody *could* have more than \$5129.1 in fringe benefits per year, how could somebody have negative valued fringe benefits?  This suggests that there is a good chance of \$0 being a "corner solution."  We can more closely at the data to see if this is in fact true.  Let's take a look at the $annbens$ variable graphically:

```{r}
fringe %>% ggplot(aes(x = annbens)) +
    geom_histogram(binwidth = 100, color = "#00573C", fill = "#00573C") +
    theme_classic() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) 

```
There are two pretty big spikes - one at zero, and another around 1000.   But these spikes are different in nature.  The presence of the big spike at 0 is suggestive of a corner solution. This next command is a little tricky, but it will ask R to list the 10 most common values for the $annbens$ variable.


```{r}
sort(table(fringe$annbens),decreasing=TRUE)[1:10]
```
Not only is 0 the most common value, it is twice as likely as any other value for this variable! This is a pretty good indication that if we want to model $annbens$, we should probably use a Tobit model.  Intuitively, the Tobit model is kind of a hybrid of OLS regression and the probit model.  In fact, it gets its name as a combination of its inventor, Nobel Laureate economist James Tobin, and the probit model.  The math behind the Tobit is essentially estimating a probit on whether or not the dependent variable is censored or not, a linear regression on the data that is not censored, and combining them into one estimate.  

A Tobit model is estimated with the `censReg()` function, which is part of the `censReg` package.  This function works a lot like the `lm()` and `glm()` commands we have been using, but we need to specify a couple extra options.  We need to tell the censReg function how the data is censored with the `left = 0` and `right = Inf` arguments.  The `Inf` is short for **infinite**, so this argument simply tells R that the data is not right-censored.  The `left = 0` tells R that the dependent variable is left censored at 0; were we dealing with right-censored data, we would use `left = -Inf` and set our right censoring with the `right` option. 

```{r, warning = FALSE}
reg1a <- censReg(annbens ~ tenure + annearn, left = 0, right = Inf, data = fringe)
stargazer(reg1a, type = "text")
```
The model estimates the value of fringe benefits as a function of an individual's tenure with their current employer and annual earnings.  We interpret the coefficients in roughly the same way as an OLS coefficient.  The $logSigma$ term is a model parameter.  One use of $logSigma$ is to exponentiate it and compare it to the standard deviation of the dependent variable:

```{r}
sd(fringe$annbens)
exp(reg1a$estimate[4])

```
The standard deviation of $annbens$ is larger than $e^{7.064}$, indicating that Tobit is likely the right class of models to use here.  

The `charity` data in the `wooldridge` package has a variable, $\mathit{gift}$, that is clearly left censored at zero. In this dataset, $\mathit{gift}$ measures the amount of money donated by 4268 potential donors.  As the histogram shows, this is a case where there are a ton of zeroes:

```{r}

charity %>% ggplot(aes(x = gift)) +
    geom_histogram(binwidth = 5, color = "#00573C", fill = "#00573C") +
    theme_classic() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) 

```
Of the 4268 observations in the dataset, over half are zero! Here are the 10 most common gift sizes in the data:

```{r}
sort(table(charity$gift),decreasing=TRUE)[1:10]

```
With this many left censored observations, Tobit is probably going to be a good model to use.

```{r, warning = FALSE}
reg2a <- censReg(gift ~ mailsyear + avggift, left = 0, right = Inf, data = charity)
stargazer(reg2a, type = "text")
```

The big difference between the $logSigma$ term to the standard deviation of $\mathit{gift}$ is suggestive that the Tobit model is the right approach:

```{r}
sd(charity$gift)
exp(reg2a$estimate[3])
```
Let's push our analysis a bit further by estimating the same model with an OLS regression using the `lm()` command.  We can compare these models side-by-side with stargazer:

```{r, warning = FALSE}
reg2b <- lm(gift ~ mailsyear + avggift, data = charity)
stargazer(reg2a, reb2b, type = "text")

```
The coefficients on these models are *very* different; the AIC and BIC are useful tools for comparing models of different types like this.  We can obtain these measures with the `AIC()` and `BIC()` commands, respectively.

```{r}
AIC(reg2b)
BIC(reg2b)
```
As lower values of AIC and BIC indicate better models, it is clear that the Tobit model outperforms the OLS model by a fairly large margin.

### Data for further exploration

* `wooldridge:mroz` - the reported age of the interviewed women, $repwage$, is left censored at zero, though this might be better modeled with a Heckman model.
* `wooldridge:jtrain2` and `wooldridge:jtrain3` might also be amenable to Heckman regression.

## The Heckman model

The Heckman model (sometimes called the Heckman Selection model) shares many similarities with the Tobit model. Like the Tobit model, the Heckman model is named for an Economics Nobel Laureate: James Heckman. At its core, it is the same combination of estimating a probit on whether or not the dependent variable is censored or not and a linear regression on the data that is not censored.  There are two key difference here.  First, in the Heckman model, the data are not "piled-up" at some value (typically on the left), they are truly unobserved.  The second difference is that we have some theory or explanation as to why some are observed and some are not.  

Let's look at some wage data, where this model is commonly used.  The `mroz` data in the `wooldridge` package looks at the wages of married women in the mid 1970s, and contains a the $repwage$ variable, which is the reported wage of the women in the data set.  If we look at at the variable more closely, we see that over half of the women have a wage of 0, indicating that they are out of the labor force. 

```{r}
?mroz
summary(mroz$repwage)

```
While they report their wage as \$0, this is not a case of these women working for free, but rather, they have opted out of the labor force.  In a sense, then, we actually don't now what their wage is/ought to be. Thus, wages are "unobserved," which is the first thing we need to estimate a Heckman model. The second thing we need is something that influences whether or not we observe their data that is not correlated with the data itself.  We will use the $kidslt6$ variable for this -- we would suspect that having more pre-school aged kids make someone less likely to work, but shouldn't necessarily reduce their wages should they choose to work.

The `selection()` command is a bit tricky, as we need to specify two models, one for the probit model to see if wages are observed or not, and a second for the regression model to actually predict wages.  We also need to edit our dataset a bit to put some `NA` values in for the zeroes.

```{r}
mroz2 <- mroz
mroz2$repwage[mroz2$repwage == 0] <- NA
```

Now that the missing values are coded as `NA` we specify the selection model as:

```{r}
reg3a <- selection(!is.na(repwage) ~ kidslt6 + educ + city + exper, repwage ~ educ + city + exper, data = mroz2)
```
The first equation models whether or not the $repwage$ variable is missing or not using the`!is.na()` command -- `is.na(repwage)` returns a yes/no on if $repwage$ is missing (`NA`), and the `!` in front of it serves as a "not" function.  This is modeled as a function of the number of children, education, experience, and a city dummy.  The second equation models $repwage$ as a function of education, experience, and the city dummy.  It is essential in a Heckman model that at least one of the variables in the selection equation is not in the OLS model.

If we use the `summary()` command, we can see both the first and second stages of the model:

```{r}
summary(reg3a)
```

The selection model is in the top table, the regression on the observed data is in the bottom table.  The $kidslt6$ variable is significant at the 2% level in the selection equation; the negative coefficient indicates the expected result that women with young children are less likely to have an observed wage.  

Let's compare these estimates with a Tobit model and an OLS regression:


```{r, warning = FALSE}
reg3b <- censReg(repwage ~ kidslt6 + educ + city + exper, data = mroz)
reg3c <- lm(repwage ~ kidslt6 + educ + city + exper, data = mroz)
stargazer(reg3a, reg3b, reg3c, type = "text")

```
We can get the AIC and BIC from the Heckman and LM models with the `AIC()` and `BIC()` functions:

```{r}
AIC(reg3a)
AIC(reg3c)
BIC(reg3a)
BIC(reg3c)
```
The Heckman model seems to be substantially better here than the Tobit model, and both are a ton better than OLS for this data.

### Data for further exploration

* Either of the data in the Tobit section of these notes -- `wooldridge:charity` or `wooldridge:fringe` -- could be looked at using the Heckman model.
* `wooldridge:jtrain2` and `wooldridge:jtrain3` might also be amenable to Heckman regression.

# Count Models

The last type of data we will look at in this notebook is count data; dependent variables that can only take on whole number values.  For example:

* The number of children an individual has
* The number of doctor's visits a person makes in a year

Typically we only use count models in cases where the counts are relatively low numbers -- a decent ballpark number is that the mean of the dependent variable should be under 10 -- We would not use a count model if our dependent variable was the number of McDonalds hamburgers sold annually at each restaurant or the number of puppies born each month by state, even though hamburgers and puppies are fundamentally countable. 
The two primary models used for count data are the **Poisson** and the **Negative Binomial** models.  The poisson model is a bit easier to estimate, so we will begin there.

We have used the binary measure from the `affairs` dataset in the `wooldridge` package before; let's now turn to the $\mathit{naffairs}$ variable, a count variable that measures the number of affairs an individual had in the past year.  First, let's look at a graph and some summary statistics of $\mathit{naffairs}$:

```{r}
summary(affairs$naffairs)
table(affairs$naffairs)
affairs %>%  ggplot(aes(x = naffairs)) +
    geom_histogram(binwidth = 1, fill = "darkblue") +
    theme_classic() +
    scale_x_continuous(expand = c(0, 0), breaks = 0:12) +
    scale_y_continuous(expand = c(0, 0)) 
    
```
The mean is 1.5, and the both the table and histogram show that the $\mathit{affairs}$ variable is indeed a count variable.  This suggests that a poisson model is likely indicated.  We estimate the poisson model with the `glm()` function and set use the `family = poisson()` option.

```{r, warning = FALSE}
reg4a <- glm(naffairs ~ male + yrsmarr + kids + relig + educ + ratemarr, data = affairs, family = poisson())
stargazer(reg4a, type = "text")

```


For the sake of comparison, we can look at these results side-by-side with an OLS estimated with the same data; Poisson estimates can be interpreted in the same way as OLS estimates:

```{r, warning = FALSE}
reg4b <- lm(naffairs ~ male + yrsmarr + kids + relig + educ + ratemarr, data = affairs)
stargazer(reg4b, reg4a, type = "text")  
```
Again, as a means of comparing models we look at the AIC: 
```{r}
AIC(reg4b)
```
THe AIC is quite a bit lower for the poisson than for the OLS estimate, indicating a better fit.  

```{r}
summary(reg4a)
```

Looking at the `summary()` of the poisson object shows us that the poisson model assumes the "(d)ispersion parameter for poisson family taken to be 1."  The `AER` package has a command called `dispersiontest()` which allows us to see if the dispersion paramener is in fact 1:

```{r}
dispersiontest(reg4a)
```
The dispersion paramenter very clearly is not equal to 1; the model is **overdispersed**, which is just a fancy way of saying that the dependent variable has more variation than the poisson model would like it to have.  The dispersion is 6.54, which is significantly different from 1. Poisson models are almost always overdispersed, and the alternatives are either the **Zero Inflated Poisson (ZIP)** or the **Negative Binomial**.  We will discuss the **Negative Binomial**, but the ZIP model will be left for the interested student to investigate further.

The **Negative Binomial** model, often referred to asa "Negbin" model, is estimated with the `glm.nb()` function in the `MASS` package. 

```{r, warning = FALSE}
reg4c <- glm.nb(naffairs ~ male + yrsmarr + kids + relig + educ + ratemarr, data = affairs)
stargazer(reg4c, type = "text")
```

Again, we can look at all 3 models together.

```{r, warning = FALSE}
stargazer(reg4a, reg4c, reg4b, type = "text")
```

The coefficients in the negbin model are similar to those in the poisson model, and the AIC is much better for the negbin than it is for the poisson, indicating it is the preferred model.  It is my experience that negbin is almost always a better choice than poisson.

### Data for further exploration

* Some good datasets looking at hospital visitation rates include `AER:Medicaid1986`, `AER:DoctorVisits`, and `AER:NMES`.
* Can you predict student attendance in  `wooldridge:attend`? This might be tricky -- don't include as independent variables anything that attendance might cause! Also, are the counts high enough that an `lm()` model is just as good?
* The `AER:EquationCitation` data might be a good place to do some basic bibliometrics.

# Wrapping Up

While models for censored and count variables are a little more niche than the OLS and probit/logit models we encountered earlier, there are some types of data for which they are indispensable; health economics and labor economics have notable examples of each, as we saw above.  

Next, we will move on to Panel regression, models where each observation is observed multiple times.
