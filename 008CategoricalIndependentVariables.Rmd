---
title: "Categorical Independent Variables"
output: html_notebook
---

# Overview

Thus far, we have only considered models in which the independent variables are numeric. Now, we expand our understanding of OLS regression to include categorical independent variables as well.  This notebook will discuss the use of dummy (indicator) variables and interaction effects in regression modelling.

Let's begin by loading in some essential packages:

```{r, warning=FALSE, message = FALSE}
library(tidyverse)
library(stargazer)
library(wooldridge)
library(AER)
data(CPS1985)
data(HousePrices)
data(diamonds)
data(gpa2)
data(TeachingRatings)


data(wage1)
data(ceosal1)
data(nyse)
data(smoke)
data(vote1)
```

# Incorporating Categorical Independent Variables

Categorical independent variables are incorporated into a regression model via a "dummy" variable.  A dummy variable is simply a variable that takes on a value of 1 if an observation is of a specific category, and 0 otherwise. For example, let's consider the CPS1985 data:

```{r}
head(CPS1985)
```
One of the variables in this data set, gender, is categorical and has two possible values (levels) in the data: male or female.

```{r}
summary(CPS1985$gender)
```
We can see that R considers this to be a factor variable:
```{r}
class(CPS1985$gender)
```
Let's transform this into a set of dummy variables.  Because our factor variable has 2 possible levels, we can make 2 different dummy variables out of it: a male dummy and a female dummy.  

```{r}
tempdata <- CPS1985
tempdata$female <- ifelse(tempdata$gender == "female", 1, 0)
tempdata$male <- ifelse(tempdata$gender == "male", 1, 0)
```
Let's verify that the dummies look right:

```{r}
tempdata[168:177, c(7, 12, 13)]
```

What if we wanted to look at a categorical variable with more than 2 levels, like occupation?

```{r}
summary(CPS1985$occupation)
```
This has 6 levels, so you would create 6 dummies.

Let's conclude this short section with a couple things worth noting.  First, in R you do not need to create dummy variables; if you run a regression with a factor variable it will do all of this in the background.  However, we will use these created dummies a few times in this notebook to see the intuition of what is going on here. Second, if a categorical variable has $L$ levels, you would only need $L-1$ dummy variables.  We will discuss why as we go along, but the reason lies in the multicollinearity assumption we saw in the previous notebook.  

Let's start by looking at some uses of dummy variables that we encountered earlier.

# Anything ANOVA Does, Regression Does Better

In some circles, those are fighting words!

In an earlier notebook, we looked at some methods of inferential statistics with categorical independent variables, particularly the 2-sample t-test and ANOVA.  Recall, the two-sample t-test is when you have a numeric dependent variable that you think varies based on the value of a categorical independent variable that has two possible levels.  An ANOVA is the same thing, but for categorical independent variables with 2 or more levels or multiple categorical variables.

```{r}
t.test(wage ~ gender, data = CPS1985)
```
```{r}
summary(aov(wage ~ gender, data = CPS1985))
```
Fun aside: there is a variant of the ANOVA called the **Welch One-Way Test** that actually gets the same results as a 2-sample t-test:

```{r}
oneway.test(wage ~ gender, data = CPS1985)
```
The same results can be arrived at via the following regression model:

\begin{equation}
Wage_{i} = \alpha + \beta female_i 
\end{equation}

```{r, warning = FALSE}
reg1a <- lm(wage ~ female, data = tempdata)
stargazer(reg1a, type = "text")
```
It may be presented slighly differently, but this result is **identical** to the ANOVA

```{r}
summary(aov(wage ~ gender, data = tempdata))
```
The F-value is identical, and if you examine the ANOVA table from the regression model, you see even more identical results:

```{r}
anova(reg1a)
```
So let's return to the actual regression results:

```{r, warning = FALSE}
stargazer(reg1a, type = "text")
```
What do these estimated coefficients mean?  The constant term is the average of the *omitted* group.  Since female is the included group, then male is the omitted group: the average male wage is \$9.995.  The $\hat{\beta}$ on female is the difference between females and males.  This result tells us that on average, females earn \$2.116 *less* than males in the data, and that this difference in wages is statistically significant.  

We can visualize this result with a boxplot:

```{r}
tempdata %>% ggplot(aes(x = gender, y = wage)) +
    geom_boxplot()

```
The regression results are telling us that the differences between these groups is in fact significant.  

The regression results stored in the object reg1a were obtained using the dummy variables we created above.  We don't actually need to create dummy variables if the data are stored as factors.  Estimating a regression using the factor variable `gender` gives the same results.

```{r, warning = FALSE}
reg1b <- lm(wage ~ gender, data = tempdata)
stargazer(reg1a, reg1b, type = "text")
```
What would have happened if we estimated a regression with the male dummies instead?

```{r, warning = FALSE}
reg1c <- lm(wage ~ male, data = tempdata)
stargazer(reg1a, reg1c, type = "text")
```
As before, the constant tells us the mean of the omitted group (female), and the $\hat{\beta}$ on male is the difference between males and females.  It should not be a surprise that it is simply the negative inverse of the female dummy from reg1a!

If you wanted to generate analogous results using female as the omitted group while using the factor variable gender, it's fairly simple using the `relevel()` function:

```{r, warning = FALSE}
reg1d <- lm(wage ~ relevel(gender, "female"), data = tempdata)
stargazer(reg1c, reg1d, type = "text")

```

Thus far, we have excluded either the male or the female dummy.  Why not simply estimate a regression with both dummies in it?

```{r, warning = FALSE}
reg1e <- lm(wage ~ male + female, data = tempdata)
stargazer(reg1e, type = "text")
```
R won't allow it, but why not?  Recall the discussion from the previous notebook regarding *multicollinearity*; we cannot have a variable that is a linear combination of other variables.  What would happen if you added the male dummy to the female dummy?

```{r}
tempdata$tempvar <- tempdata$male + tempdata$female
tempdata[168:177, c(7, 12, 13, 14)]
```
It seems that you will get a column of 1s, and a column of 1s will be collinear with the variable that is added to the regression to calculate the constant term.  If you *really* wanted, you could run this regression with no constant term:

```{r, warning = FALSE}
reg1f <- lm(wage ~ female + male - 1, data = tempdata)
stargazer(reg1f, type = "text")
```
But there is really no point in doing so.  You don't get any added information, and if your research question is about the difference between the male and female groups, this regression doesn't tell you that.  You know that $\hat{\beta_1} =7.879$ and $\hat{\beta_2}=9.995$ are significantly different from zero, but you don't know if the difference between $\hat{\beta_1}$ and $\hat{\beta_2}$ is significantly different.

In this data, gender was a factor variable with 2 levels--what if we have a factor with more than 2 levels, say occupation? We can call a regression model in much the same way:

```{r, warning = FALSE}
reg2a <- lm(wage ~ occupation, data = tempdata)
stargazer(reg2a, type = "text")
```
Here, constant is the average wage for the omitted group (worker), and the $\hat{\beta}$ estimates for the various occupations are interpreted relative to the omitted group; for example, people whose occupation is "management" make on average \$4.28 more than those whose occupation is "worker", and that difference is statistically significant.  While mathematically it does not matter which occupation is the omitted group, when doing hypothesis testing it is often most useful to omit the group with either the highest or lowest average.  

```{r, warning = "FALSE"}
reg2b <- lm(wage ~ relevel(occupation, "services"), data = tempdata)
stargazer(reg2b, type = "text")
```

The releveled variables are a bit ugly in the table, this is because we releveled the factor variable inside the `lm` command.  We could also permanently relevel the variable with the relevel command and get cleaner looking regression tables:

```{r, warning = FALSE}
tempdata$occupation <- relevel(tempdata$occupation, "services")
reg2c <- lm(wage ~ occupation, data = tempdata)
stargazer(reg2c, type = "text")
```


Just like with ANOVA, we can look at interaction effects with categorical variables as well.  The syntax is the same as before:

```{r, warning = FALSE}
reg3a <- lm(wage ~ gender*married, data = tempdata)
stargazer(reg3a, type = "text")
```

This result is a bit trickier to interpret, so let's start with examining the regression model being estimated:

\begin{equation}
Wage_i = \alpha + \beta_1 female_i +\beta_2 married_i +\beta_3 (female_i \: and \: married_i)
\end{equation}

Essentially, we have 4 groups

* married female
* unmarried female
* married male
* unmarried male

For each group, the values of the dummies would look like:

```{r, echo = FALSE}
temptable <- matrix(c("category", "unmarried male", "married male", "unmarried female", "married female", "female", 0, 0, 1, 1, "married", 0, 1, 0, 1, "married and female", 0 ,0, 0, 1), nrow =5, ncol = 4)
knitr::kable(temptable)
```

In this model, the omitted group would be the group of individuals who have a 0 for each of the values of $X$, so unmarried males are the omitted group, and the constant is their mean. To make predictions, you simply add in the value of $\beta$ for any of the categories a particular individual belongs to.  So how do we interpret the estimated coefficients?

```{r, warning = FALSE}

stargazer(reg3a, type = "text")
```

The ones on gender and marital status are fairly straigtforward; they state that women make on average 9 cents less than men (this result is not statistically significant), and that married people make on average \$2.52 more than unmarried people (this result is statistically significant). Interpreting the interaction term is a bit trickier, and how you do it depends on your hypothesis.  To illustrate this idea, the following two statements are both supported by the regression (and are actually saying the same thing!):

* The effect of gender on wages is \$3.10 bigger for married women than it is for single women.
* THe effect of being married on wages is \$3.10 bigger for men than it is for women.

All of the results so far are identical to those we could have obtained using ANOVA; so why is regression superior to ANOVA?  Because ANOVA can only handle categorical independent variables, but regression can easily incorporate both into the same model!

# Combining Numeric and Categorical Independent Variables

Let's continue with the CPS1985 data and consider the following model:

\begin{equation}
Wage_i = \alpha + \beta_1 education_i +\beta_2 female_i
\end{equation}

Education is a numeric variable while female is categorical.  The resulting regression looks like:

```{r, warning = FALSE}
reg4a <- lm(wage ~ education + gender, data = CPS1985)
stargazer(reg4a, type = "text")
```

In this model, $\hat{\beta_1}=0.751$ tells us that an additional year of education is correlated with an increase in wages of \$0.75 per hour.  This point estimate does not vary between genders.  The estimated $\hat{\beta_2} = -2.124$ suggests that, for a given amount of education, females are predicted to earn \$2.12 less than a male.  Essentially, this model is estimating 2 regression lines, one for males and another for females, but both lines have the same slope.  Graphically, this model looks something like:

```{r}
CPS1985 %>% ggplot(aes(x = education, y = wage, color = gender)) +
    geom_point() +
    scale_color_manual(values = c("darkgreen","orange")) +
    geom_abline(slope = .751, intercept = .218, color = "darkgreen") +
    geom_abline(slope = .751, intercept = .218-2.124, color = "orange") +
    theme_classic()
```
This is an important tool, so let's work through a few more examples, starting with the HousePrices data in the AER package:

```{r}
head(HousePrices)
```
This table has data on houses sold in Windsor, Ontario (Canada) in the summer of 1987.  The data include the sales price, along with a number of attributes of each house sold, and gives us a great set of data to estimate what is called a **hedonic regression**.  Hedonic regression is a model that attempts to predict the market value of set of goods based on the attributes of those goods, and is useful for pricing decisions.  Let's take a look at the data: 

```{r}
head(HousePrices)
```
```{r}
summary(HousePrices)
```
Let's use price as the dependent variable, and use all of the house characteristic variables.

```{r, warning = FALSE}
reg4b <- lm(price ~ lotsize + bedrooms + bathrooms + stories + driveway + recreation + fullbase + gasheat + aircon + garage + prefer, data = HousePrices)
stargazer(reg4b, type = "text")
```
These results have many useful, real-world applications. For example, let's say it is the summer of 1987 and you have a house in Windsor, Ontario that you would like to sell...what is a good guess of the market price?  You simply enter the attributes into the regression equation and create a prediction.  For example, let's say your house has the following characteristics:

* 5000 sqft lot 
* 4 bedrooms
* 2 bathrooms
* 2 stories
* driveway
* rec room
* no basement
* no gas heating
* no air conditioning
* 2 car garage
* preferred neighborhood

We can take these attributes and put them into a new `data.frame` with the same variable names as in HousePrices:

```{r}
yourhouse <- data.frame(lotsize = 5000, bedrooms = 4, bathrooms = 2, stories = 2, driveway = "yes", recreation = "yes", fullbase = "no", gasheat = "no", aircon = "no", garage = 2, prefer = "yes")
```
Next, we can use the `predict` function and use our regression object but feed it in our custom house using the `newdata` option:

```{r}
predict(reg4b, newdata = yourhouse)
```
Such a house is predicted to sell for \$91,864.  Let's take this logic a little further.  Let's say that our real estate agent tells us that if we want top dollar for our home, we should  finish our basement, add a bedroom and a bathroom, and upgrade our HVAC to have gas heating and air conditioning.  So we get a quote for all this work and it's going to cost \$35,000.  Is it worth it?

Let's start by duplicating row 1 of the yourhouse dataframe into row 2:

```{r}
yourhouse[2,] <- yourhouse[1,]
```
Next, let's edit row 2 to have the new attributes:

```{r}
yourhouse[2,]$bedrooms <- 5
yourhouse[2,]$bathrooms <- 3
yourhouse[2,]$fullbase <- "yes"
yourhouse[2,]$gasheat <- "yes"
yourhouse[2,]$aircon <- "yes"
```
Finally, let's rerun our prediction command:

```{r}
predict(reg4b, newdata = yourhouse)
```
It looks like those additions would add approximately \$47,000 in value to our house, so perhaps the updates are worth doing!

Let's look at another dataset that is prime for hedonic regression, the `diamonds` dataset that comes in the *ggplot* package:

```{r}
head(diamonds)
summary(diamonds)
```
This dataset includes information about over 50,000 round cut diamonds; let's use a hedonic pricing model to estimate diamond price as a function of the 4 Cs of diamonds:

* Carat
* Cut
* Color
* Clarity

Carat is the weight of the diamond, so it is a number.  However, cut, color, and clarity are all categorical (factor) variables with lots of levels, so this is going to be a big dummy variable regression.

For ease of reading the output tables, we will first force R to read the categorical variables as unordered.

```{r, warning = FALSE}
diamonds2 <- as.data.frame(diamonds)
diamonds2$cut <- factor(diamonds2$cut, ordered = FALSE)
diamonds2$color <- factor(diamonds2$color, ordered = FALSE)
diamonds2$clarity <- factor(diamonds2$clarity, ordered = FALSE)
diamonds2$color <- relevel(diamonds2$color, c("J")) 
reg4c <- lm(price ~ carat + cut + color + clarity, data = diamonds2)
stargazer(reg4c, type = "text")
```
Could we predict the price of a 32 carat, D color, internally flawless with ideal cut diamond?  Mathematically, sure we could:

```{r}
mydiamond <- data.frame(carat = 32, clarity = "IF", cut = "Ideal", color = "D")
predict(reg4c, newdata = mydiamond)
```
But statistically this is a bad idea.  If we look at a summary of the data:

```{r}
summary(diamonds2$carat)
```
We see that the biggest diamond in the data set was only 5.01 carats. Thus, making a prediction about a 32 carat diamond is beyond the scope of the model, as the prediction is trying to estimate a point outside of the range of the sample data.  This is called *extrapolation* and is generally frowned upon; we may be fairly sure that the relationship between size and price is linear in the range we are looking at, there is no guarantee that it is linear beyond the range of the data (and in the case of diamonds, it almost certainly is not!)

```{r, fig.align = "center", echo=FALSE, fig.cap="XKCD: Extrapolating", out.width = '100%'}
knitr::include_graphics("images/extrapolating.png")
```

Turning away from hedonic regression, let's look at the gpa2 data in the wooldridge package.  Let's model GPA as a function of SAT score, an athlete dummy, and a gender dummy. Note that this data set has the athlete and gender variables coded as dummies already, so we are not dealing with factor variables.   

```{r, warning = FALSE}
reg4d <- lm(colgpa ~ sat + athlete + female, data = gpa2)
stargazer(reg4d, type = "text")
```

These results suggest:

* For each additional 100 points on the SAT, the model predicts a (statistically significant) .2 increase in GPA.  This effect is assumed to be the same for males and females, athletes and non-athletes.
* The average GPA among athletes is 0.02 higher than for non-athletes.  THis is not statistically significant.
* Females have on average a .23 higher GPA than males. This difference is statistically significant.

Lets try this again with a slight variation: here we include a female athlete interaction effect.

```{r, warning = FALSE}
reg4e <- lm(colgpa ~ sat + athlete*female, data = gpa2)
stargazer(reg4d, reg4e, type = "text")
```

Judging from the lack of significance of the interaction term and the fact that the adjusted $R^2$ did not move, it seems like there is no interaction effect.  This implies that the effect of gender on grades is not affected by one's status of being an athlete (or equivalently, the effect of being an athlete is not different between males and females).

Let's examine one more example involving interaction effects before moving on with the *TeachingRatings* data set from the AER package. We will estimate a regression looking at the relationship between student evaluation of instructors and the attractiveness of instructors, interacted dummies for minority and female faculty, a native english speaker dummy, and a dummy to indicate whether or not the class is a single credit elective course (e.g. yoga)

```{r, warning = FALSE}
reg4f <- lm(eval~beauty + gender + minority + native + credits, data = TeachingRatings)
reg4g <- lm(eval~beauty + gender*minority + native + credits, data = TeachingRatings)
stargazer(reg4f, reg4g, type = "text")
```

Let's look specifically at the interacted variables.  The function of an interaction effect is really to give an "it depends" sort of answer.  So if I were to ask the question, do minority faculty receive lower teaching evaluations, the answer to that question is that it depends on if whether or not that faculty is male or female.  A minority male would have a 1 for the minority dummy, but a 0 for the interaction dummy, so in aggregate he would be expected to receive a teaching evaluation, all else equal, that is .003 lower than the omitted group (here, that is non-minority males). However, a minority female would receive a 1 for the minority dummy, the female dummy, AND the interaction dummy, so she would be expected to receive a teaching evaluation (again all else equal) .414 lower than a non-minority male.  

# Interactions with Numeric Variables

In the final section of the notebook on dummy variables, we will look at interactions with numeric variables.  Interpreting these can be tricky.  