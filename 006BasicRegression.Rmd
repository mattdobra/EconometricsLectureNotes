---
title: "Basic Regression"
output: html_notebook
---

# Overview

Regression modeling is at the center of econometric methods.  For most of the rest of the semester, we will be estimating various types of regression.  We begin with the simplest of regressions; this set of notes will look at ordinary least squares (OLS) modeling.  

As usual, let's start by loading the libraries and data we will be using in this notebook:

```{r, warning=FALSE, message = FALSE}
library(tidyverse)
library(stargazer)
library(AER)
data(CPS1985)
attach(CPS1985)
```

# Prelude to Regression: The Correlation Test

We saw how to calculate a correlation (Pearson's r) in a previous notebook; let's see the correlation between wage and years of education from the use the CPS1985 dataset using `cor()` function

```{r}
cor(wage, education)
```

Pearson's r looks at the strength of the linear relationship between two variables; graphically, it looks something like this:

```{r, message = FALSE}
CPS1985 %>% ggplot(aes(x = education, y = wage)) +
    geom_point() +
    geom_smooth(method = lm)
```
The correlation is a measure of the strength on the linear relationship between the x variable (education) and the y variable (wage).  If the correlation coefficient were 1, we could say that these variables had a perfect positive linear relationship,  If it were 0, we would say that these variables are unrelated.  So is .38 statistically significant? 

* $H_0$: The true correlation coefficient is 0
* $H_1$: The true correlation coefficient is not 0

We can test this hypothesis with the function `cor.test()`:

```{r}
cor.test(education, wage)
```
This indicates that the correlation coefficient is in fact significantly different from 0.  

Let's look at the correlation between years of experience and wage.  It is useful to start by looking at a graph of the data:

```{r, message = FALSE}
CPS1985 %>% ggplot(aes(x = experience, y = wage)) +
    geom_point() +
    geom_smooth(method = lm)
```
This looks a lot like a horizontal line, indicating there is probably no relationship between experience and wage.  Let's run the test, just to be sure:

```{r}
cor.test(wage, experience)
```
The p-value is 0.044, which is barely less than 0.05.  So again we reject the null hypothesis.  Perhaps it is a good thing we ran the test rather than simply look at the graph!

Another type of correlation is the Spearman correlation; typically, you might want to use the Spearman test rather than the Pearson test if you have skewed data or outliers.  Looking at the graph, it's probably the case that income data is skewed right:

```{r}
CPS1985 %>% ggplot(aes(x = wage)) +
    geom_histogram(binwidth = 2)
```
So let's take a look at the Spearman test results:

```{r}
cor.test(wage, experience, method = "spearman", exact = FALSE)
```
The Spearman statistic $\rho = 0.17$ is significantly different from 0.  Because this data is skewed, the Spearman statistic is probably more accurate than the Pearson.

# Ordinary Least Squares

The most basic regression model is referred to as Ordinary Least Squares, or OLS.  As this model forms the basis of all that is to follow in this class, it is worth spending some time to develop a good intuition of what is going on here. It is easiest to start with the bivariate model, as we can develop the intuition graphically as well as mathematically.

## OLS Intuition

In a simple, bivariate OLS, we are looking at the **linear** relationship between our **dependent variable** and *one* **independent variable**.  Suppose we are looking at the relationship between years of education and wages, where education is our independent variable, X, and wages are our dependent variable, Y.  To say this model is linear implies that the relationship between X and Y looks like:

\begin{equation}
Y_{i} = \alpha + \beta X_{i} 
\end{equation}

That is, for any value of $X_i$, years of education, you can determine that person's wage by multiplying their years of education by $\beta$ and then adding $\alpha$.  Graphically, this relationship looks like:

```{r, echo=FALSE, warning=FALSE}
library(broom)
set.seed(5)
y <-rnorm(5)
x <-1:5
mod <- lm(y ~ x)
df <- augment(mod)
ggplot(df) + 
  geom_line(aes(x = x, y = .fitted), size = 1) +
  geom_segment(aes(x = 1, y = -1, xend = 5, yend = -1), color = "black") +
  geom_segment(aes(x = 1, y = -1, xend = 1, yend = 1), color = "black") +
  geom_point(aes(x = 1, y = -0.54), size = 5, color = "orange") +
  geom_point(aes(x = 1.95, y = -0.2), size = 5, color = "purple") +
  geom_point(aes(x = 4, y = 0.58), size = 5, color = "purple") +
  geom_segment(aes(x = 4, y = 0.58, xend = 4, yend = -0.2), color = "purple", size = 0.5) +
  geom_segment(aes(x = 1.95, y = -0.2, xend = 4, yend = -0.2), color = "purple", size = 0.5) +
  annotate("text", x = 1.15, y = -0.56, label = expression(alpha), color = "orange", size = 6) +
  annotate("text", x = 4.2, y = 0.2, label = expression(Delta*"Y"), color = "purple", size = 6) +
  annotate("text", x = 3, y = -0.3, label = expression(Delta*"X"), color = "purple", size = 6) +
  annotate("text", x = 4, y = -0.3, label = expression(beta*" = "*Delta*"Y/"*Delta*"X"), size = 6, color = "darkgreen" ) +
  annotate("text", x = 2.2, y = 0.5, label = expression("Y = "*alpha + beta*"X"[1]), color = "black", size = 7) +
  lims(y = c(-1, 1), x = c(1, 5)) + 
  labs(x = expression("X"), y = expression("Y")) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```
This is an example of a model that is **fully determined** -- there is no scope for individuals to not be on the line.  If, for example, $\beta = 1$ and $\alpha = 0.5$, then we would say that someone with 7 years of education should have an hourly wage of \$7.50, someone with 13 years an hourly wage of \$13.50, and so forth.  The real world is rarely so neat and tidy, and a regression is an example of a *stochastic* model.  

In a stochastic model, we use statistical tools to approximate the line that best fits the data.  That is, we are still trying to figure out what $\alpha$ and $\beta$ are, but we are aware that not everybody will be on the line that is implied by $\alpha$ and $\beta$.  This means that the model looks more like:

\begin{equation}
Y_{i} = \alpha + \beta X_{i} + \epsilon_i
\end{equation}

The new term, $\epsilon_i$, is called the **error term** or the **residual**.  Where does the error term come from?  We take our values of \alpha and \beta and combine them with $X_i$ to calculate a predicted value of Y.  Typically we refer to the predicted value of Y as $\hat{Y}$ (pronounced Y-hat).  The difference between the actual value of $Y_i$ and $\hat{Y}$ is $\epsilon_i$:

\begin{equation}
\epsilon_{i} = \hat{Y} - Y_i
\end{equation}

The theory (and math) behind a regression model focuses on trying to estimate values for $\alpha$ and $\beta$ that best fits the observed data. Because we don't know $\alpha$ and $\beta$, we estimate $\hat{\alpha}$ and $\hat{\beta}$ with our regression.  

In a regression model, we estimate a model that looks like: 

\begin{equation}
Y_{i} = \hat{\alpha} + \hat{\beta} X_{i}+ \epsilon_{i} 
\end{equation}

In this model, $\epsilon_i$ is referred to as our **residual**, which is just the vertical distance each individual data point is from the line implied by $\hat{\alpha}$ and $\hat{\beta}$.

Perhaps this is easier to understand if looked at graphically:

```{r, echo=FALSE}
library(broom)
set.seed(5)
y <-rnorm(5)
x <-1:5
mod <- lm(y ~ x)
yl <- c("y[1]", "y[2]", "y[3]", "y[4]", "y[5]")
yhl <- c("hat(y)[1]", "hat(y)[2]", "hat(y)[3]", "hat(y)[4]", "hat(y)[5]")
sl <- c("hat(epsilon)[1]", "hat(epsilon)[2]", "hat(epsilon)[3]", "hat(epsilon)[4]", "hat(epsilon)[5]")
df <- augment(mod)
ggplot(df) + 
  geom_segment(aes(x = x, y = y, xend = x, yend = .fitted), linetype = "dashed", color = "purple") +
  geom_point(aes(x = x, y = y), size = 5, color = "purple") +
  geom_line(aes(x = x, y = .fitted)) +
  geom_point(aes(x = x, y = .fitted), size = 5) +
  geom_text(aes(x = x, y = y), label = yl, color = "purple", size = 2, parse = TRUE) +
  geom_text(aes(x = x, y = .fitted), label = yhl, color = "purple", size = 2, parse = TRUE) +
  geom_text(aes(x = x + 0.2, y = (y + .fitted) / 2), label = sl, color = "black", size = 2, parse = TRUE) +
  ylim(-1.5, 2) + 
  theme(axis.text = element_blank())
```
Assume that the black line is the one implied by our estimates of $\hat{\alpha}$ and $\hat{\beta}$, and each purple point represents one of our data poins.  The vertical distance each point is from the line is its residual $\epsilon_i$.

So where do $\hat{\alpha}$ and $\hat{\beta}$ come from?  How do we determine the line of best fit?  The best fitting line is the one where the values of $\hat{\alpha}$ and $\hat{\beta}$ minimize the sum of squared errors, $\Sigma{\epsilon_i^2}$.  This is why the method is called Ordinary Least **Squares**! In principle, we could take a data set, run it through every single possible combination of $\hat{\alpha}$ and $\hat{\beta}$, calculate, square, and sum all the values of $\epsilon_i$ that result from those values of $\hat{\alpha}$ and $\hat{\beta}$, and choose the $\hat{\alpha}$ and $\hat{\beta}$ that minimize $\Sigma{\epsilon_i^2}$.  There is a much cleaner way of doing this, but it involves a fair bit of calculus so it will be left as an exercise for the interested student to research on his/her own.  

## Estimating and Interpreting a Bivariate Regression

### Estimating the Model

Let's look again at the CPS1985 dataset and the relationship between education and wage.  Again, the scatter plot looks like:

```{r, message = FALSE}
CPS1985 %>% ggplot(aes(x = education, y = wage)) +
    geom_point() 
```

We can estimate a regression model between wage and education with the `lm()` command, If it helps you remember the command, `lm()` stands for **linear model**. Because we attached the CPS1985 dataset, we could simply use the command `lm(wage ~ education)` if we wanted, but here I will explicitly tell R which data to use.

```{r}
lm(wage ~ education, data = CPS1985)
```
### Obtaining Useful Results

The immediate output from a regression model is not very helpful; it tells you the values of $\hat{\alpha}$ and $\hat{\beta}$, but that's it.  You pretty much always want to to store your model as an object and inspect the object separately. 

```{r, warning = FALSE}
reg1 <- lm(wage ~ education)
```

We can use the `attributes()` command to see what is contained within this object; we will make use of some of these shortly.  
```{r}
attributes(reg1)
```
To interpret the model, we need to take a deeper look at this regression object.  The Base R method is to use the `summary()` command, though I prefer `stargazer` because it's cleaner and the format looks like the way regressions get published in academic journals. 

```{r}
summary(reg1)
```

```{r, warning = FALSE}
stargazer(reg1, type = "text")
```

Now this is a lot more useful.  There's lots of stuff in here.  The values of $\hat{\alpha}$ and $\hat{\beta}$ are found in the coefficients panel--the estimate of the Intercept/constant is $\hat{\alpha}$ and the estimated coefficient on education is $\hat{\beta}$.  This means that our regression model is in fact:

\begin{equation}
\hat{Wage_{i}} = -0.74598 + 0.75046 Education_{i} 
\end{equation}

### Predicted Values and Residuals

We can use this equation to make predictions: the predicted wages of somebody with 12 years of education is:

\begin{equation}
\hat{Wage_{i}} = -0.74598 + 0.75046 * 12
\end{equation}

\begin{equation}
\hat{Wage_{i}} = \$8.26
\end{equation}

Recall above that we used the `attributes(reg1)` command and saw that we can grab the model's residuals, $\epsilon_i$, and fitted values, $\hat{Y_1}$.  This next code clones the CPS1985 data into a new dataset called tempdata, attaches the residuals and fitted values to that new dataset, and gets rid of all the variables we didn't use in our regression. Here are the first 10 observations in the data set, along with their predicted values and residuals:


```{r}
resids <- reg1$residuals
preds <- reg1$fitted.values
tempdata <- cbind(CPS1985, preds, resids)
tempdata %>% 
    select(c(wage,education, preds, resids)) %>% 
    slice(1:10)
```

Recall, wage is our dependent variable ($Y$), and education is our independent variable ($X$).  The preds variable records the values of $\hat{Y}$ for each level of education $X_i$ and the resids variable records how far from the predicted wage $\hat{Y}$ each of the actual wagee $Y_i$ is.

### Look at the stars, look how they shine for you

As an aside, Coldplay sucks. Rather than think about their excrementary music, let's look back at the `stargazer()` output so we can think happy thoughts again:

```{r, warning = FALSE}
stargazer(reg1, type = "text")
```

You should have noticed all the stars next to the education coefficient.  The the stars are very useful for interpreting the **statistical significance** of your regression model.  In a regression, the null hypothesis is that the true coefficients ($\beta$ and $\alpha$) are equal to zero, the alternative hypothesis is that they are not equal to zero.  For the most part, the significance of $\alpha$ doesn't matter; we focus on $\beta$ and whether or not our estimate of $\hat{\beta}$ is significantly different from zero.

Why zero?  If $\beta > 0$, higher values of $X_i$ are associated with higher values of $Y_i$.  If $\beta < 0$, higher values of $X_i$ are associated with lower values of $Y_i$.  If $\beta = 0$, then there is no relationship between our dependent variable and our independent variable.  In other words, our null hypothesis is that X and Y are unrelated, and by rejecting the null hypothesis we are saying that we believe that X and Y are in fact related.   

The bottom panel shows useful information about the regression.  Observations is your sample size, and R2 is actually $R^2$ (pronounced R-squared) and is a measure of goodness of fit.  The possible range for$R^2$ is $0 \geq R^2 \geq 1$.  In a bivariate model, $R^2$ is actually the correlation coefficient squared!

```{r}
cor(wage, education)
cor(wage, education)^2
```

$R^2$ is often called the **coefficient of determination**, and, while not technically true, is easiest thought of as the percentage of the variation in $Y$ that you can explain with your independent variable $X$.  Our $R^2=0.146$ then might be interpreted as saying that we can explain roughly 15% of the variation in wages with education, and therefore the other 85% of the variation in wages is left unexplained.  

The F-test is a measure of the significance of the entire model--the null hypothesis is that every $\hat{\beta}$ that you estimated is equal to zero.  In a bivariate model, there is only $\hat{\beta}$ so the F-test of the model is basically the same thing as the t-test of $\hat{\beta}$.  

### Visualizing the Regression Line

Using `ggplot()`, it is easy to add the regression line to a scatterplot by adding the `geom_smooth(method = lm)` argument. 

```{r, message = FALSE}
CPS1985 %>% ggplot(aes(x = education, y = wage)) +
    geom_point() +
    geom_smooth(method = lm,
                se = FALSE)
```

## More Examples of Bivariate Regression

Now that we have the basics of regression down, let's grab a few more datasets from the wooldridge library and practice running and interpreting regressions.  We will add more nuance to our interpretation as we go.

```{r}
library(wooldridge)
data(airfare)
data(gpa2)
data(vote1)
data(meap01)
```

Let's start with the airfare data and examine the relationship between the average one-way fare and the distance in miles of the routes.  Which variable should be dependent and which should be independent?  Statistics cannot prove causation, that is the role of (economic) theory. But when constructing our regressions, we should be thinking about economic logic and how it informs our thinking about causality. 

```{r, fig.align = "center", echo=FALSE, fig.cap="XKCD: Correlation vs Causation", out.width = '100%'}
knitr::include_graphics("images/correlation.png")
```

We can start by looking at a scatterplot:

```{r, message = FALSE}
airfare %>% ggplot(aes(x = dist, y = fare)) +
    geom_point() + 
    geom_smooth(method=lm)
```
Next, we can estimate the regression model using the `lm()` command.  Because I didn't attach the dataset, I need to tell the `lm()` function where the variables are.  One option would be to use the \$ notation, as in `lm(airfare$fare ~ airfare$dist)`, though I prefer the `lm(fare ~ dist, data = airfare)` syntax because I stopped paying attention in typing class when we got to the section on symbols and I always make typos when I type \$:

```{r}
reg2 <- lm(fare ~ dist, data = airfare)
```

The regression output is stored in the object *reg2*, so let's use `stargazer()` to check it out. The `stargazer()` function defaults to output in $\LaTeX$, so you may want to stick with the `type = "text"` option. If you want to make prettier versions of `stargazer()` output to cut and paste into MS Word or the like, you might want to look into how to use the `type = "HTML` option and how to save `stargazer()` output.  

```{r, warning = FALSE}
stargazer(reg2, type = "text")
```
The coefficient on the distance variable is significant at the 99% level and $R^2 = .39$, both of which indicate strong statistical significance.  Let's think about what this means in terms of our regression equation: 

\begin{equation}
fare_{i} = 103.261 + 0.076 distance_{i} 
\end{equation}

The $\beta$ implies that, if distance increases by 1, then fare increases by 0.076.  What are the units of measure here?  Distance is measured in miles, and fare is measured in \$US. So this equation suggests that, on average, a one mile increase in the distance of a flight is associated with a \$0.076 (7.6 cent) increase in price.  This isn't really an intuitive unit of measure--it may be more intuitive to think of a 100 mile increase in distance is associated with a \$7.60 increase in price. 

Let's take this logic a little further and start to learn a bit about data transformation. 

```{r}
airfaretemp <- airfare %>% 
    select(fare, dist) %>% 
    mutate(dist100 = dist/100)
airfaretemp[c(1,5,9,13,17),]
```
This bit of code creates a new dataset called airfaretemp and adds a new variable called dist100.  The variable dist100 is created by dividing the dist variable by 100, so really it's just measuring distance in hundreds of miles.  I also had R spit out 5 lines of data just to confirm that $dist100=\frac{dist}{100}$. Now, let's estimate the regression between fare and dist100 and put it side by side with our original regression:

```{r, warning = FALSE}
reg2a <- lm(fare ~ dist100, data = airfaretemp)
stargazer(reg2, reg2a, type = "text")
```
This is pretty cool--the results are more or less identical, except for the decimal place in the distance variable.  And we get the same intuitive result we saw above.

Next, let's take a look at the gpa2 data. This data includes student data from a midsize research university.  Let's look at the relationship between a student's SAT score and his/her GPA after the fall semester:

```{r, message = FALSE}
gpa2 %>%  ggplot(aes(x = sat, y = colgpa)) +
    geom_point() +
    geom_smooth(method = lm)
```
Next, store the regression in an object and look at it using `stargazer()`
```{r, warning = FALSE}
reg3 <- lm(colgpa ~ sat, data = gpa2)
stargazer(reg3, type = "text")
```
Here, $R^2 = .17$ and our estimate of $\hat{\beta}$ is significant at the 99% level.  We can interpret the coefficient of .002 as stating that we expect GPA go go up by .002 on average for every 1 point of SAT.  This is a bit of an awkward interpretation, notably because SAT scores don't go up by 1, they go up by 10!  We can use the same trick as above to scale these numbers in a way that is more meaningful; a 100 point increase in overall SAT score is associated with a 0.2 increase in GPA seems a lot more meangingful.  

Let's look at the estimated value of $\hat{\alpha}$ of 0.663.  What does that mean, and what does it mean that it is significant?  Let's start with the question of what it means and look at the regression equation:

\begin{equation}
gpa_{i} = 0.663 + 0.002 sat_{i} 
\end{equation}

What would our expected gpa, $\hat{gpa}$, be for a student who earned a 0 on the SAT?  Anything multiplied by 0 is 0, so 0.663 is our estimated GPA for somebody who earned a 0 on the SAT.  But is it even possible to earn a 0 on the SAT? They give you 400 points just for showing up.  So looking at a 0 on the SAT is somewhat meaningless in this model, because it is impossible to even get.  The $\alpha$ in the equation is valuable for calculating predicted values, but in most cases, $\hat{\alpha}$ is not really what we are looking at in a regression model.  This brings us to the question of the significance of the $\hat{\alpha}$ term - the fact that we are pretty darn sure that the true value of $\alpha$ is not really 0 doesn't really mean anything here.  

Most of the time, the value of $\hat{\alpha}$ is not of terrible importance, though here is a counterexample: consider the CAPM model, a standard model in finance that you hopefully are at least aware of.  The CAPM model can be written as:

\begin{equation}
(R-R_f)=\alpha + \beta(R_m-R_f) 
\end{equation}

Where:

* $R$ is the rate of return of an asset or portfolio
* $R_f$ is the risk-free rate of return
* $R_m$ is the market rate of return
* $\beta$ is the relative volatility of the asset 
* $\alpha$ is a risk-adjusted rate of return: the extent to which the asset under- or over-performed the market when taking into consideration the riskiness of the asset

The CAPM model can be estimated using bivariate regression, and when people in finance talk about stock market betas, they are literally talking about $\beta$ from a CAPM regression!  In this model, you may in fact not only be interested to know the value of $\alpha$, but whether or not it is significantly different from zero is of importance as well!

Getting the data for this will be a bit tricky; to follow along you will need to install the `tidyquant` and `quantmod` packages -- `install.packages("tidyquant")` should install both for you.  

Let's get some data:

```{r}
library(quantmod)
library(tidyquant)

firstsolar <- tq_get('FSLR', # firstsolar Ticker
               from = "2017-01-01",
               to = "2018-01-01",
               get = "stock.prices")

vanguard500 <- tq_get('VFINX',  # vanguard500 Ticker
               from = "2017-01-01",
               to = "2018-01-01",
               get = "stock.prices")

coke <- tq_get('KO',  # Coke Ticker
               from = "2017-01-01",
               to = "2018-01-01",
               get = "stock.prices")

spx <- tq_get('^GSPC',  # S&P 500 Ticker
               from = "2017-01-01",
               to = "2018-01-01",
               get = "stock.prices")

tbill <- tq_get('^IRX', # 90 day T-bill
               from = "2017-01-01",
               to = "2018-01-01",
               get = "stock.prices")

# Convert stocks prices into daily rates of return

firstsolar <- firstsolar %>% 
    select(date, adjusted) %>% 
    mutate(lnadjusted = log(adjusted)) %>% 
    mutate(firstsolar = lnadjusted - lag(lnadjusted)) %>% 
    select(date, firstsolar)

vanguard500 <- vanguard500 %>% 
    select(date, adjusted) %>% 
    mutate(lnadjusted = log(adjusted)) %>% 
    mutate(vanguard500 = lnadjusted - lag(lnadjusted)) %>% 
    select(date, vanguard500)

coke <- coke %>% 
    select(date, adjusted) %>% 
    mutate(lnadjusted = log(adjusted)) %>% 
    mutate(coke = lnadjusted - lag(lnadjusted)) %>% 
    select(date, coke)

spx <- spx %>% 
    select(date, adjusted) %>% 
    mutate(lnadjusted = log(adjusted)) %>% 
    mutate(spx = lnadjusted - lag(lnadjusted)) %>% 
    select(date, spx)

# t bill quotes are annualized, convert to daily

tbill <- tbill %>% 
    select(date, adjusted) %>% 
    mutate(tbill = adjusted/360) %>% 
    select(date, tbill)

# Combine all the datasets

returns <- spx 
returns <- merge(returns, vanguard500, by = "date")
returns <- merge(returns, firstsolar, by = "date")
returns <- merge(returns, coke, by = "date")
returns <- merge(returns, tbill, by = "date")
returns <- drop_na(returns)
```

This downloads stock ticker data for First Solar (FSLR), Vanguard 500 (VFINX), Coca-Cola (KO), and S&P 500 (SPX), and the 3 month T-bill rate (which will be used for our risk-free rate).  Next, we estimate 3 CAPM regressions; one for Coke, one for First Solar, and one for the Vanguard 500, and display them in `stargazer()`.

```{r, warning = FALSE}
capm1 <- lm(coke-tbill~spx-tbill, data = returns)
capm2 <- lm(firstsolar-tbill~spx-tbill, data = returns)
capm3 <- lm(vanguard500 - tbill ~ spx - tbill, data = returns)

stargazer(capm1, capm2, capm3, type = "text")
```
This is a model that we actually care about the value of $\alpha$, which represents the risk-adjusted rate of return.  This suggests that both coke and the Vanguard 500 had significantly negative alphas.  In actuality, this is probably not true--the asset returns are calculated daily but the risk adjusted return used is a 3 month return--it would be better to have data with matching time horizons, but I digress.  

It might also be of note here that the significance test of our estimated $\hat{\beta}$ values is relative to $\beta = 0$, but in this particular application of financial econometrics, you may not be interested in $\beta = 0$, you may be more interested in a hypothesis of whether or not the asset has equal volatility to the market ($\beta=1$), or is it more ($\beta>1$) or less ($\beta<1$) volatile than the market.

The easiest way to get insight into this question is to use the `confint` command to generate a confidence interval for the regression model:

```{r}
confint(capm1) # Coke Model
```
We see that our confidence interval for our first regression (for Coca-Cola stock) states that we are 95% confident that $-0.005<\beta<.336$, indicating that we are fairly certain that Coca-Cola stock is far less volatile than the market as a whole.  Repeating this process for the First Solar stock and the Vanguard 500 fund:

```{r}
confint(capm2) # First Solar Model
confint(capm3) # Vanguard Model
```
We see that the 95% confidence interval in the First Solar regression is $1.229<\beta<2.873$, indicating that the finding that this stock is more volatile than the market is statistically significant at the 95% level, and we see that, unsurprisingly, the Vanguard regression shows $0.981<\beta<1.020$ so $\hat{\beta}$ is not significantly different from 0. 

Let's return to some simpler examples, starting with the meap01 data; this data has school level funding and test score data in the state of Michigan.  Does spending per student lead to better performing students?  We can look at the relationship between the variable math4, the percent of students receiving a satisfactory 4th grade math score, and exppp, expenditures per pupil. We start by looking at a plot of the data:

```{r, message = FALSE}
meap01 %>% ggplot(aes(x = exppp, y = math4)) +
    geom_point(color = "goldenrod",
               alpha = .4) +
    geom_smooth(method = lm)
```
If you have read a paper or two in the economics of education literature, the slope of that line shouldn't come as a surprise to you.  Let's estimate a regression:

```{r, warning = FALSE}
reg4 <- lm(math4 ~ exppp, data = meap01)
stargazer(reg4, type  = "text")
```
The sign on the $\hat{\beta}$ is negative, which corresponds to the downward sloping line in the plot.  However, there are no stars next to the coefficient, implying that the result is not significantly different from 0; thus, even though $\hat{\beta} < 0$, we do not reject the null hypothesis that $\beta = 0$.

Finally, let's take a look at the voting data in the vote1 dataset.  Specifically, let's examine the relationship between the share of campaign expenditures (shareA) a candidate made and the vote share received (voteA). We can start with a graph:

```{r, message = FALSE}
vote1 %>% ggplot(aes(x = shareA, y = voteA)) +
    geom_point() +
    geom_smooth(method = lm)
```
Next, we estimate the regression and put the results into `stargazer()*` for ease of viewing. 

```{r, warning = FALSE}
reg5 <- lm(voteA ~ shareA, data = vote1)
stargazer(reg5, type = "text")
```
The $R^2$ is pretty huge $R^2 = 0.86$ and the $\hat{\beta}$ is positive and significant at the 99% level.  This is a tricky $\beta$ to interpret, because the variable shareA is defined as $\frac{Expenditures\:by\: Candiate\:A}{Expenditures\:by\:Candidate\:A + Expenditures\:by\:Candidate\:B}$. It can be done, but it is tricky.  In some cases, it is easier just interpreting the **signs** and **significance** of the coefficients, and not focusing so much on the actual **magnitude** of the coefficients.  

# Multivariate Regression

## Predictive modeling and hypothesis testing

We can extend this logic into having multiple independent variables.  Multivariate regression is a bit harder to wrap one's mind around, because it is really difficult to graph (adding variables adds dimensions to the graph, so we are no longer dealing with a 2d space), but the basics of interpreting the variables is the same as with bivariate modeling.  Why would we want to estimate a multivariate regression?  It depends on if your goal is ultimately prediction or hypothesis testing, but either way, nearly every application of regression is a multivariate application.

The goal of regression analysis is typically **prediction** or **hypothesis testing**.  Within the context of a typical regression model $Y = \alpha +\beta_1X_1+\beta_2X_2+...+\beta_kX_k$:

* **Predictive** modeling refers to a situation where your primary purpose is to predict outcome $Y$ as accurately as possible.  Statistics and data analytics tend to focus on predictive modeling.
* **Hypothesis testing** refers to a model where your primary purpose is to predict the impact of a particular independent variable (e.g. $X_1$) or set of independent variables (e.g. $X_1$, $X_2$, and $X_3$) on the outcome $Y$. Such a variable is commonly referred to as a **variable of interest**; that is, the whole purpose of running the regression is that we are interested in the coefficient on that variable. Economics tends to focus on hypothesis testing.

In either case, one should always include as independent variables **all** variables that they think might impact the value of the dependent variable, even if it is not the variable of interest.  

### Predictive modeling

For predictive modeling, the reason why should be clear--If you are trying to predict $Y$, leaving out relevant information will ultimately lead to less precise and accurate predictions.  As an example, imagine trying to predict wages using only a person's age.  Age probably  matters for wages, but without knowing information like years of education, which also probably matters, your predictions are going to be less accurate.  

Let's look at this example using the CPS1985 data.  Column 1 has a regression with age as the only $X$ variable, column 2 includes both age and education.

```{r, warning = FALSE}
reg6 <- lm(wage ~ age, data = CPS1985)
reg7 <- lm(wage ~ education + age, data = CPS1985)
stargazer(reg6, reg7, type = "text")
```

To see which has more accurate predictions, let's make a boxplot of the residuals of both models.  Recall that the residual is equal to $\epsilon_i=Y_1-\hat{y}$, and as $\hat{y}$ is our predicted value, bigger values of $\epsilon_i$ implies less accurate predictions.

```{r}
boxplot(resid(reg6), resid(reg7))
```

Close inspection of the two boxplots shows that the one of the right has less spread, and thus more predictive accuracy.  In fact, the standard deviations of the residuals from the two models are:

```{r}
sd(resid(reg6))
sd(resid(reg7))
```

### Hypothesis testing

Hypothesis testing typically focuses on attempting to estimate the "true" relationship between one or more independent variables of interest and the dependent variable.  One might naturally wonder, why worry about other variables if you are only interested in one specific $\hat{\beta}$?  The answer lies in developing a bit deeper intuition of how the regression math works.  Ordinary least squares works by attributing the variation in the dependent variable Y to the variation in your independent variables.  Consider the model $Y=\alpha +\beta X + \epsilon$: it turns out that if you go through the calculus behind OLS, you will find that $\beta=\frac{cov(X,Y)}{var(Y)}$!

The big concern here is that maybe your variable of interest ($X_1$) is correlated (**multicollinear**) with another variable not in your model ($X_2$), so your estimate of $\hat{\beta}$ will include both the direct effect of $X_1$ ***and*** the indirect effect of $X_2$ to the extent $X_2$ is correlated with $X_1$.  THis is referred to as **omitted variable bias**, which we will discuss more in the next notebook.  For the purposes of hypothesis testing, however, if what we really care about is the *true* effect of $X_1$ on $Y$, then we need to include any other variables ($X_2$, $X_3$, etc) that we also think might have an effect on $Y$ as well.

## Example of multivariate regression

Let's do a basic example. Using the CPS1985 data, what happens when we include education AND age in the same regression.  

```{r, warning = FALSE}
reg6 <- lm(wage ~ age, data = CPS1985)
reg7 <- lm(wage ~ education + age, data = CPS1985)
stargazer(reg1, reg6, reg7, type = "text")
```

The first column has the first regression in this notebook.  The second looks at the relationship between age and wage, and the third column looks at the model with both education and age. What observations might we make when we consider these regressions side by side by side?

* The $\hat{\beta}$ estimates in column 3 are not equal to those in column 1 or 2.  As noted before, this is expected and is probably a sign that column 3 is a better model the other 2.
* The $R^2$ in the multivariate model is larger than in either of the bivariate models.  This *by definition* has to be true.  Recall that our coefficient of determination $R^2$ roughly measures the percentage of variation in $Y$ that is explained by our dependent variable(s).  In other words, how much of the variation in $Y$ is explained by the **information** contained with in our $X$ variables. Adding more $X$ variables does not remove information, it only potentially adds information. If I added a variable that had no explanatory power to this model, I would expect $R^2$ to not move at all.
* So why not just add a bunch of variables?  If $R^2$ can only go up, why not run a **kitchen sink** regression and include every variable I can think of?  This results in a problem called **overfitting**, and to avoid this we look to the adjusted $R^2$.  Adjusted $R^2$ is simply $R^2$ but adds a penalty for the number of independent variables included in the model.  Basically, the idea is to only add anothe variable to your model if that variable adds a significant amount of explanatory power. Say you estimate a multivariate model and are wondering if you should add another variable.  By definition, adding that variable will cause $R^2$ to go up.  But if adding that variable causes adjusted $R^2$ to fall, then you shouldn't do it, because adding the variable does not add enough information to the model to justify its inclusion.
* We interpret the coefficients in the same way as in a bivariate model - column 3 implies that a 1 year increase in education is associated with a \$0.82 increase in hourly wage, and a 1 year increase in age is associated with a \$0.11 increase in wage.  It is always useful to keep in mind the economic notion of *ceteris parebus* here!
* We can make predictions the same way as well.  If somebody has 16 years of education and is 34 years old, the model predicts that their wage will be $-\$5.53 + \$0.82(16) + \$0.11(34) = \$11.33$. Even if a variable is not significant, we need to include it in our math to make predictions.

# Wrapping Up

At this point, you should be able to use R to estimate a simple bivariate or multivariate OLS regression and to interpret the regression output.  We will next turn to some of the key assumptions of regression analysis.